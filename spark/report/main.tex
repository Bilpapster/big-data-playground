\documentclass[acmlarge]{acmart}



\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
\settopmatter{printacmref=false}

\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{geometry}
\newcommand{\todo}{{\color{red}\textbf{TODO} }}
\newcommand{\disease}{{\small \texttt{DISEASE}} }
\newcommand{\hospital}{{\small \texttt{HOSPITAL}} }
\newcommand{\storage}{{\small \texttt{STORAGE}} }

% christos 2, 4
% vasilis 1, 3

\usepackage{amsmath}
\usepackage{subfig}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocodex}
\newcommand{\algorithmautorefname}{Algorithm}

\begin{document}

  \title{Big Data Analytics with Scala and Apache Spark}
  \subtitle{M.Sc. course on ``Technologies for Big Data Analysis'' - Assignment 3}

  \author{Christos Balaktsis (506)}
  \email{balaktsis@csd.auth.gr}
  \author{Vasileios Papastergios (505)}
  \email{papster@csd.auth.gr}
  \affiliation{
    \institution{Aristotle University}
    \city{Thessaloniki}
    \country{Greece}
  }

  \renewcommand{\shortauthors}{C. Balaktsis and V. Papastergios}
  \maketitle

  \section{Introduction}

  The current document is a technical report for the third programming assignment in the M.Sc. course on
  \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc Program}\footnote{https://dws.csd.auth.gr/} of the Aristotle University of Thessaloniki, Greece. The course is taught by Professor Apostolos Papadopoulos~\footnote{https://datalab-old.csd.auth.gr/$\sim$apostol/}. The authors attended the course during their first year of Ph.D. studies at the Institution.

  The assignment contains 4 sub-problems and is part of a series, comprising 3 programming assignments on the following topics:
  \begin{description}
    \item[Assignment 1] Multi-threading Programming and Inter-Process Communication
    \item[Assignment 2] The Map-Reduce Programming Paradigm
    \item[Assignment 3] Big Data Analytics with Scala and Apache Spark
  \end{description}
  In this document we focus on Assignment 3 and its 4 sub-problems.
  We refer to them as \emph{problems} in the rest of the document for simplicity.
  The source code of our solution has been made available at the following public repository in the GitHub platform: \href{https://github.com/Bilpapster/big-data-playground}{\texttt{\small https://github.com/Bilpapster/big-data-playground}}.

  \textbf{Roadmap}.
  The rest of our work is structured as follows.
  We devote one section to each one of the 4 problems.
  That means problems 1, 2, 3 and 4 are presented in~\autoref{sec:problem1},~\autoref{sec:problem2},~\autoref{sec:problem3} and~\autoref{sec:problem4} respectively.
  For each problem, we first provide the problem statement, as given by the assignment.
  Next, we thoroughly present the reasoning and/or methodology we have adopted to approach the problem and devise a solution.
  Wherever applicable, we also provide insights about the source code implementation we have developed.
  Finally, we conclude our work in~\autoref{sec:conclusion}.
  The appendix includes the evaluation results for any issues that necessitated them.

  \section{Problem 1: Word Length Analytics}
  \label{sec:problem1}
  We discuss here the first problem of the assignment.
  The main target of the assignment is to get familiar with a simple task leveraging Apache Spark and Scala programming language. This is a WordCount problem's variation.

  \subsection{Problem Statement}
  Implement an \textbf{Apache Spark} (Scala) program, a variation of the \textbf{word-count} problem, to compute the average length of words that start with a specific letter (a-z). The program should sort the results based on the average length, printing first the letters with higher average length. For example:
  \begin{description}
    \item k – $8.2$
    \item a – $5.6$
    \item b – $4.8$
  \end{description}
  Note that you may opt for preprocessing the input text, e.g., transforming all letters to lowercase or ignoring words that start with a number.

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task1WordLength.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task1} directory.

  \subsubsection{Implementation}
  The proposed approach leverages the Apache Spark computing framework to calculate the average word length for each one of the 26 letters in the Latin alphabet. In particular, the input text is read and tokenized into words. As part of a preprocessing step, all letters are transformed to lowercase and all words that start with a number are dropped.

  In order to compute the average length for each initial letter, we leverage the \texttt{RDD} API of Apache Spark. More specifically, we use a \texttt{PairRDD} that stores composite key-value pairs in the form \texttt{(letter, (word\_length, 1))}. The key of each pair is the initial letter of the respective word (\texttt{letter}). The value of each pair is a pair itself, having as (false) key the length of the word (\texttt{word\_length}) and as (false) value a unary value (\texttt{1}). We use the term \emph{false} for the key and value of the inner pair, since we do not utilize them as an actual key-value pair. As a matter of fact, we treat the inner pair as a plain tuple of values,.

  Subsequently, we group all pairs by key (i.e., initial letter) and compute the following two intermediate results \emph{per initial letter}:
  \begin{enumerate}
    \item sum of word lengths that start with the respective letter, by adding up all false keys of the inner pair, and
    \item total number of words that start with the respective letter, by adding up all unary values (false values) of the inner pair.
  \end{enumerate}

  Having computed these intermediate results for each initial letter, the average length can be computed as the fraction
  \begin{displaymath}
    average~word~length_{~letter} = \frac{sum~of~word~lengths_{~letter}}{total~number~of~words_{~letter}},~ \forall letter \in Latin~Alphabet
  \end{displaymath}

  As a final step, the results are sorted in descending order w.r.t. the average length and saved to a text file. By default, after successful execution of the program, the results can be found under the \texttt{output/task1} directory.

  \subsubsection{Evaluation}
  The experiments were executed using the dataset \texttt{SherlockHolmes.txt}. The results are listed below in a 10-column format and graphically depicted in~\autoref{img:histogram:word_lengths}.
  \begin{multicols}{10}
    \noindent
    c:7.19
    \\ e: 7.11
    \\ q: 7.01
    \\ p: 7.01
    \\ r: 6.87
    \\ d: 6.37
    \\ v: 5.98
    \\ g: 5.93
    \\ s: 5.8
    \\ z: 5.74
    \\ u: 5.61
    \\ j: 5.59
    \\ k: 5.37
    \\ l: 5.34
    \\ f: 5.17
    \\ m: 5.14
    \\ n: 4.81
    \\ b: 4.54
    \\ w: 4.28
    \\ h: 3.81
    \\ a: 3.74
    \\ y: 3.72
    \\ t: 3.64
    \\ i: 3.46
    \\ x: 3.41
    \\ o: 3.01
    \label{multicol:test}
  \end{multicols}

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/download}
    \caption{A histogram depicting the average word length (y-axis) for every letter of the Latin alphabet (x-axis). The mean average length is shown as a horizontal red dashed line.}
    \label{img:histogram:word_lengths}
  \end{figure}

  \section{Problem 2: Airline Tweets analytics}
  \label{sec:problem2}
  The second problem focuses on producing analytic insights from an Twitter feedback post for airline services.


  \subsection{Problem Statement}
  In this task, we take on the role of a data analyst aiming to help airlines improve the quality of their services. Our data source is a CSV file (each row corresponds to a tweet) containing Twitter comments about airline services, with the following format:

  \begin{itemize}
    \item \texttt{tweet\_id}
    \item \texttt{airline\_sentiment}
    \item \texttt{airline\_sentiment\_confidence}
    \item \texttt{negativereason}
    \item \texttt{negativereason\_confidence}
    \item \texttt{airline}
    \item \texttt{name}
    \item \texttt{text}
    \item \texttt{tweet\_created}
    \item \texttt{user\_timezone}
  \end{itemize}

  You are required to create a program using Spark DataFrames to answer the following questions:

  \begin{enumerate}
    \item \textbf{What are the 5 words in the tweet text that appear most frequently for each \texttt{airline\_sentiment} category: positive, negative, and neutral?}
    \item \textbf{What is the main reason for complaint (\texttt{negativereason}) for each airline, i.e., the reason associated with the most tweets?} Consider only tweets with \texttt{negativereason\_confidence > 0.5}.
  \end{enumerate}

  In the implementation, you should ignore punctuation marks, and consider all words to be in lowercase for the related processing.

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task2AirlineTweets.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task2} directory.

  \subsubsection{Implementation}
  The implementation utilizes Apache Spark for distributed processing, ensuring scalability for large datasets. The process is divided into two main tasks, described in detail below.
  \\

  \textit{Task 1: Extracting Top Words by Sentiment}
  \\

  \textbf{Preprocessing:}
  \begin{enumerate}
    \item Text was converted to lowercase to ensure case insensitivity.
    \item Non-alphabetic characters were removed using regular expressions, eliminating punctuation and special symbols.
  \end{enumerate}

  \textbf{Word Frequency Analysis:}
  \begin{enumerate}
    \item Tweets were filtered based on their sentiment (\texttt{airline\_sentiment} = positive, negative, or neutral).
    \item Tokenization was performed to split the cleaned text into individual words.
    \item A word frequency distribution was computed by grouping words and counting their occurrences.
    \item The top 5 most frequent words for each sentiment category were extracted and ranked.
  \end{enumerate}

  \textbf{Output:}
  Results were saved in separate directories (\texttt{output/task2/\{sentiment\}-top-words}) for each sentiment category as CSV files.
  \\

  \textit{Task 2: Identifying Main Complaint Reasons}
  \\

  \textbf{Filtering:}
  \begin{enumerate}
    \item Tweets with a non-null \texttt{negativereason} field were selected.
    \item Only tweets with \texttt{negativereason\_confidence} > 0.5 were considered to ensure reliable complaint reasons.
  \end{enumerate}

  \textbf{Grouping and Ranking:}
  \begin{enumerate}
    \item Tweets were grouped by \texttt{airline} and \texttt{negativereason}.
    \item The frequency of each complaint reason was calculated for each airline.
    \item A ranking function (\texttt{row\_number}) was applied within each airline's group to identify the most frequent complaint reason.
  \end{enumerate}

  \textbf{Output:}
  The results, including the airline, top complaint reason, and its frequency, were saved in a CSV file (\texttt{output/task2/top-complaints}).

  \paragraph{Implementation Details}
  The implementation uses the following key components of Apache Spark:
  \begin{itemize}
    \item \textbf{SparkSession:} Initializes the Spark application and provides the entry point for DataFrame operations.
    \item \textbf{DataFrame API:} Used for transformations, including filtering, grouping, and ranking.
    \item \textbf{Regular Expressions:} Applied to clean the text data by removing non-alphabetic characters.
    \item \textbf{Window Functions:} Enabled ranking of complaint reasons for each airline.
  \end{itemize}



  \subsubsection{Evaluation} Our solution is tested using the provided tweets dataset, namely \texttt{tweets.csv}. The results for each analytical query are presented below.

  \begin{table}[h!]
    \centering
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        the & 903 \\ \hline
        to & 880 \\ \hline
        you & 811 \\ \hline
        for & 628 \\ \hline
        thanks & 587 \\ \hline
      \end{tabular}
      \caption{Top-5 positive words}
    \end{minipage} \hspace{0.3cm}
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        to & 1573 \\ \hline
        i & 1126 \\ \hline
        the & 927 \\ \hline
        a & 774 \\ \hline
        united & 700 \\ \hline
      \end{tabular}
      \caption{Top-5 neutral words}
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        to & 5686 \\ \hline
        the & 3914 \\ \hline
        i & 3396 \\ \hline
        a & 3033 \\ \hline
        flight &2763 \\ \hline
      \end{tabular}
      \caption{Top-5 negative words}
    \end{minipage}
  \end{table}

  \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Airline} & \textbf{Reason} & \textbf{Count} \\ \hline
      American & Customer Service Issue & 654  \\ \hline
      Delta & Late Flight & 228 \\ \hline
      Southwest & Customer Service Issue & 323 \\ \hline
      US Airways & Customer Service Issue & 698 \\ \hline
      United & Customer Service Issue & 545 \\ \hline
      Virgin America & Customer Service Issue & 51 \\ \hline
    \end{tabular}
    \caption{Top complaints for each Airline}
  \end{table}


  \section{Problem 3: Movie Analytics}
  \label{sec:problem3}
  The third problem focuses on producing analytic insights into a movies dataset.


  \subsection{Problem Statement}
  In this problem, you are provided with a dataset that contains information about movies. Each record represents a movie and includes three main columns (attributes), namely \texttt{movieId}, \texttt{title} and \texttt{genres}. An example record of the dataset is the following:

  \begin{verbatim}
1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy
  \end{verbatim}

  Note that a movie can belong to multiple genres (e.g., adventure, fantasy, and so on). The genres are separated by a vertical line (`|') in the respective field. Your Apache Spark (Scala) application should compute the following analytics:
  \begin{enumerate}
    \item \textbf{How many movies are there for each genre?} If a movie belongs to multiple genres, it should be counted to all these genres. Sort the results by the name of genre in alphabetical order.
    \item \textbf{How many movies have been filmed per year?} Note that the year a movie was filmed is currently encoded into a composite movie title, e.g., ``\texttt{Toy Story (1995)}''. Show the top 10 years with the most movies filmed within them.
    \item \textbf{Which are the words that appear at least 10 times in the titles of the movies, and what is their total frequency?} You can ignore words that have less than 4 characters. Sort the results, showing first the words with the higher frequency.
  \end{enumerate}

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task3MovieAnalytics.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task3} directory.

  \subsubsection{Implementation} We leverage Apache Spark \emph{DataFrames} API to calculate the aforementioned analytical queries. Our proposed approach comprises the following four stages:
  \begin{description}
    \item[Stage 1:] Data cleaning and preprocessing
    \item[Stage 2:] Computation of genre analytics (query 1)
    \item[Stage 3:] Computation of year analytics (query 2)
    \item[Stage 4:] Computation of title analytics (query 3)
  \end{description}
  \autoref{img:movieAnalyticsSolutionDiagram} graphically depicts the four stages, as well as their assembly in a workflow. We elaborate on each stage separately in the rest of the Subsection.

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/movieAnalytics}
    \caption{The MapReduce solution architecture for the movie analytics problem. Two separate jobs are executed to solve the two tasks, namely duration per country (green) and movies per year \& genre (yellow). The two tasks also create separate subdirectories for writing the results.}
    \label{img:movieAnalyticsSolutionDiagram}
  \end{figure}

  \paragraph{Stage1: Data Cleaning and preprocessing} In this stage, we address data quality issues that are present in the raw movie data. Missing production year, missing genres and trailing whitespaces in the title string are the detected data quality issues. To address these issues, we explicitly mark the movie year as ``N/A'' (not available) \& movie genre as ``(no genre listed)'' and trim the composite title strings, respectively. Subsequently, we transform the data representation to a more efficient form, by extracting the actual title and year to separate fields (instead of a composite one) and splitting the genres composite string field to an array of genres. Throughout the whole process, we \textbf{use solely the DataFrame API} of Apache Spark, leveraging also the pattern matching functionality of Scala. A small sample of the cleaned and transformed DataFrame shown in~\autoref{img:movieAnalyticsSolutionDiagram} is depicted below:

  \begin{verbatim}
                  +-------+--------------------+--------------------+----+
                  |movieId|               title|              genres|year|
                  +-------+--------------------+--------------------+----+
                  |      1|           Toy Story|[Adventure, Anima...|1995|
                  |      2|             Jumanji|[Adventure, Child...|1995|
                  |      3|    Grumpier Old Men|   [Comedy, Romance]|1995|
                  |      4|   Waiting to Exhale|[Comedy, Drama, R...|1995|
                  |      5|Father of the Bri...|            [Comedy]|1995|
                  +-------+--------------------+--------------------+----+
  \end{verbatim}

  \paragraph{Stage 2: Computation of genre analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie genres. In particular, we use the DataFrame API (\texttt{select()}, \texttt{group\_by()}, etc.) to calculate the number of movies that belong to each genre. An implementation detail of our solution lies in handling the cases where a movie belongs to multiple genres. To ensure the integrity of the computed analytics, we opt for using the \texttt{explode()} function of the DataFrame API, to transform the array of genres to separate genre items. The analytics are, then, computed on top of this extra transformation, enabling us to include \textbf{a movie into the calculations of all the genres it belongs}.

  \paragraph{Stage 3: Computation of year analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie production years. Similarly to Stage 2, we leverage exclusively the DataFrame API to calculate the total number of movies filmed each year. In this case, the \texttt{group\_by} operation is more straightforward, since the cleaned and transformed DataFrame contains a dedicated column for the movie production year.

  \paragraph{Stage 4: Computation of title analytics} Similarly to Stage 2, we use operations available from the DataFrame API to calculate the most frequently used words in the movie titles. We, leverage the \texttt{split(" ")} and \texttt{explode()} functions to transform titles, first, into an array of words and, then, into separate words that are finally grouped and aggregated. We remove words that have less than 4 characters and fundamental stopwords, such as ``with'', or ``from'', in order to obtain analytical insights of high quality.

  \subsubsection{Evaluation} Our solution is tested using the raw movie data, namely \texttt{movies.csv}. We list the execution results in~\autoref{sec:data3}.

  \paragraph{Stage 2: Computation of genre analytics}
  We list here the execution results for the genre analytics query in a 6-column format.
  \begin{multicols}{6}
    \noindent
    Action: 7348
    \\ Adventure: 4145
    \\ Animation: 2929
    \\ Children: 2935
    \\ Comedy: 16870
    \\ Crime: 5319
    \\ Documentary: 5605
    \\ Drama: 25606
    \\ Fantasy: 2731
    \\ Film-Noir: 353
    \\ Horror: 5989
    \\ IMAX: 195
    \\ Musical: 1054
    \\ Mystery: 2925
    \\ Romance: 7719
    \\ Sci-Fi: 3595
    \\ Thriller: 8654
    \\ War: 1874
    \\ Western: 1399
    \\ N/A: 5062
  \end{multicols}

  \paragraph{Stage 3: Computation of year analytics}
  We list here the execution results for the year analytics query in a 3-column format, i.e., the top 10 years with the most movies. We observe that these years span the time period between 2009 and 2018, excluding 2015, indicating a heavy film production process in this decade. \autoref{img:movies_per_year} depicts the distribution of movies per year for all years in chronological order. Our findings indicate a clear long-tail distribution of movies over the years, with a burst of new movies over the last decade.
  \begin{multicols}{3}
    \centering
    \noindent
    2015: 2512 movies
    \\ 2016: 2488 movies
    \\ 2014: 2406 movies
    \\ 2017: 2373 movies
    \\ 2013: 2173 movies
    \\ 2018: 2032 movies
    \\ 2012: 1978 movies
    \\ 2011: 1838 movies
    \\ 2009: 1724 movies
    \\ 2010: 1691 movies
  \end{multicols}

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/movies per year}
    \caption{The distribution of the total number of movies per year in the dataset used for evaluating our solution.}
    \label{img:movies_per_year}
  \end{figure}

  \paragraph{Stage 4: Computation of title analytics}
  We list in~\autoref{sec:data3} the execution results for the title analytics query.Also, ~\autoref{img:word_cloud} depicts a word cloud with the most frequently used words in movie titles, found in the dataset used for evaluating our solution. Note that the bigger a word is, the more frequently it appears in movie titles.
  \begin{figure}[tb!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/word cloud}
    \caption{A word cloud depicting the most frequent words appearing in the titles of the movies in the dataset examined. The bigger a word is, the more frequently it appears in the titles of the movies.}
    \label{img:word_cloud}
  \end{figure}

  \section{Problem 4: Probabilistic graph}
  \label{sec:problem4}
  We discuss here the fourth problem of the assignment.
  The main target of the assignment is to get acquainted with performing various MapReduce phases on a pipeline frame.

  \subsection{Problem Statement}
  We are given an input file in text format where each line contains a connection between two vertices of a network and a
  probability value.
  For each edge \( e \), there is a probability value \( p(e) \) which indicates the probability that the two vertices are
  connected by the edge.
  Obviously, the values of \( p(e) \) range between 0 and 1.
  The values in each line are separated by a space.
  Consider the network shown in the previous figure.
  The edge connecting vertices 4 and 5 has a probability of
  0.8, the edge connecting vertices 2 and 3 has a probability of 0.2, etc.

  \begin{figure}[h]
    \centering
    \includegraphics[width=0.25\linewidth]{figures/graph}
  \end{figure}

  The file corresponding to this graph would be:

  \[
    1 \ 2 \ 0.6, \\
    1 \ 3 \ 0.9, \\
    2 \ 3 \ 0.2, \\
    3 \ 4 \ 0.75, \\
    2 \ 5 \ 0.2, \\
    4 \ 5 \ 0.8,
  \]

  The edges are generally stored in random order in the file, so we cannot assume they have a specific arrangement.

  The following tasks are requested:

  \begin{enumerate}
    \item Write a Java program that computes the average degree for all the vertices.
    \item The average degree is defined as the sum of the probabilities of the edges that fall on a vertex.
    \item For example, in the previous diagram, the average degree of vertex 3 is \( 0.9 + 0.2 + 0.75 = 1.85 \).
    \item Before performing this calculation, you should ignore all edges with a probability less than a threshold \( T \)
    , which should be passed as a parameter to the main function.
    \item Modify the code from task 1 so that, at the end, only the vertices with an average degree greater than the
    average of the degrees of all the vertices are displayed in the output.

  \end{enumerate}

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
  We have used the Java Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

  The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
  the repository.
  The project is compiled and executed directly from IntelliJ IDEA.

  To run the project, open the \texttt{GraphMaster} class in IntelliJ IDEA, and execute the main method.
  You will need to specify the following command-line arguments:

  \begin{itemize}
    \item \texttt{<input\_path>} specifies the directory containing the input text files, located at
    \texttt{map-reduce/ probabilisticGraph /input}.
    \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be
    created in the \texttt{out} folder.
    \item \texttt{<T>} is the minimum edge-degree threshold for vertices to be included in the results.
  \end{itemize}

  IntelliJ IDEA will handle the compilation and execution automatically when the main method is run.
  Make sure to configure the input and output paths as required for your specific run.

  Note that the command-line arguments must follow the specified order and format.
  If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

  \subsubsection{Implementation}
  The problem at hand involves processing a probabilistic graph, where each edge connects two vertices with a probability
  value.
  The task is to compute the average degree for each vertex, considering only edges whose probability exceeds a given
  threshold \( T \).
  The methodology proposed here utilizes a distributed MapReduce framework, implemented using Hadoop, to efficiently
  process and analyze large-scale graph data.
  The approach is divided into three main phases, each handled by separate MapReduce jobs, with intermediate results
  passed between the phases.

  The \texttt{GraphMaster} class serves as the orchestrator of the entire MapReduce process.
  It manages the execution of the three phases, invoking the appropriate MapReduce jobs in sequence.
  The main steps executed by \texttt{GraphMaster} are:

  \begin{enumerate}
    \item It retrieves the command-line arguments, including the input and output directories as well as the threshold
    value \( T \), which determines which edges to consider in the graph.
    \item It initiates the first MapReduce job to compute the degree of each vertex in the graph.
    The input consists of edge data, and the output is a summation of edge probabilities for each vertex.
    \item After the first job completes, the \texttt{GraphMaster} starts the second MapReduce job to calculate the mean
    degree of the graph, using the results from the first job.
    \item Once the mean degree is computed, \texttt{GraphMaster} starts the third MapReduce job, which filters out the
    vertices with degrees lower than the mean degree.
    \item It cleans up intermediate directories after each phase and moves the final output to the desired location.
  \end{enumerate}

  \texttt{GraphMaster} coordinates these steps by configuring and executing the MapReduce jobs, ensuring that each phase
  depends on the results of the previous one. \\

  The entire process consists of three MapReduce jobs, executed sequentially:

  \begin{enumerate}
    \item \textbf{Phase 1:} The first job computes the degree of each vertex by summing the probabilities of the edges
    connected to it.
    It produces intermediate results for each vertex.
    \item \textbf{Phase 2:} The second job calculates the mean degree by summing the degrees from the first phase and
    dividing by the total number of vertices.
    \item \textbf{Phase 3:} The third job filters out the vertices with a degree lower than the mean degree and produces
    the final filtered result.
  \end{enumerate}

  After each job completes, intermediate data is written to disk and passed as input to the subsequent job.
  The final output consists of the vertices whose degree exceeds or equals the mean degree. \\

  \textbf{Phase 1: Calculating Vertex Degree}
  The first MapReduce job is responsible for calculating the degree of each vertex in the graph, where the degree is
  defined as the sum of the probabilities of the edges connected to that vertex.
  The degree of each vertex is computed by the \texttt{GraphMapper} and \texttt{GraphReducer} classes.

  The \texttt{GraphMapper} class reads each edge from the input data, which consists of lines containing two vertices
  and the associated probability value.
  It splits each line into two components (the two vertices), and for each vertex, it emits the corresponding probability
  value as the output.
  The mapper also checks if the probability value exceeds the threshold \( T \), which is provided as a configuration
  parameter.
  If the probability is greater than or equal to \( T \), it emits the vertex along with the corresponding probability.

  The \texttt{GraphReducer} class receives the vertex and associated probability values emitted by the mapper.
  For each vertex, it sums up all the probabilities of the edges connected to that vertex, and emits the pair as output.

  \textbf{Phase 2: Calculating the Mean Degree}
  The second phase of the approach calculates the mean degree of all vertices.
  This is done by summing the degree values from the previous phase and dividing by the total number of vertices.

  The \texttt{MeanMapper} class receives the degree values from the output of the first MapReduce job.
  It emits two key-value pairs for each input line: a count of the number of vertices and the sum of the degree values.
  These are emitted with fixed keys (\texttt{"count"} and \texttt{"sum"}) so that they can be aggregated in the reducer.

  The \texttt{MeanReducer} class processes the count and sum values emitted by the mapper.
  It calculates the total sum and count of vertices and then writes these values as output.
  The mean degree is calculated by dividing the sum by the count, and the result is stored for use in the next phase.

  \textbf{Phase 3: Filtering Vertices by Mean Degree}
  In the third phase, the vertices whose degree is greater than or equal to the mean degree calculated in Phase 2 are
  selected.
  This phase uses the results from Phase 1 and Phase 2, applying the filtering criteria to retain only those vertices with
  a degree greater than or equal to the mean.

  The \texttt{FilterMapper} class takes the degree values emitted by the first reducer and writes them as key-value pairs.
  Each key is a vertex, and the value is the degree of that vertex.
  The mapper does not perform any filtering; instead, it simply passes the values along to the reducer.

  The \texttt{FilterReducer} class filters out the vertices whose degree is less than the mean degree.
  It retrieves the mean degree from the configuration, and for each vertex, it compares the degree value to the mean.
  If the degree is greater than or equal to the mean, it emits the vertex along with its degree.
  This final output contains only the vertices that satisfy the filtering criteria.

  \subsubsection{Evaluation}
  The experiments were executed using the dataset `collins.txt` with a parameter setting of \( T = 0.8 \).

  To provide a comprehensive overview, the results of the experiment, including the detailed values derived from the
  dataset, are presented in~\autoref{sec:data4} in a 2-column format.



  \section{Conclusion}
  \label{sec:conclusion}
  In this document we have presented our solutions and rationale for solving the second assignment of the M.Sc.
  course on \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc. Program}. For each one of the four problems of the assignment, we have presented their statement, as well as the solution approach we have adopted. All execution results with the provided datasets can be found in the appendices of our work.

  \newpage
  \appendix

  \section{Problem 2 execution results}
  \label{sec:data2}
  \input{appendix2}

  \section{Problem 3 execution results}
  \label{sec:data3}
  \input{appendix3}

  \subsection{Problem 4 execution results}
  \label{sec:data4}
  \input{appendix4}


\end{document}
\endinput

