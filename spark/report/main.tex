\documentclass[acmlarge]{acmart}



\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
  \settopmatter{printacmref=false}

\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{geometry}
\newcommand{\todo}{{\color{red}\textbf{TODO} }}
\newcommand{\disease}{{\small \texttt{DISEASE}} }
\newcommand{\hospital}{{\small \texttt{HOSPITAL}} }
\newcommand{\storage}{{\small \texttt{STORAGE}} }

% christos 2, 4
% vasilis 1, 3

\usepackage{amsmath}
\usepackage{subfig}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocodex}
\newcommand{\algorithmautorefname}{Algorithm}

\begin{document}

\title{Big Data Analytics with Scala and Apache Spark}
\subtitle{M.Sc. course on ``Technologies for Big Data Analysis'' - Assignment 3}

\author{Christos Balaktsis (506)}
\email{balaktsis@csd.auth.gr}
\author{Vasileios Papastergios (505)}
\email{papster@csd.auth.gr}
\affiliation{
  \institution{Aristotle University}
  \city{Thessaloniki}
  \country{Greece}
}

\renewcommand{\shortauthors}{C. Balaktsis and V. Papastergios}
\maketitle

\section{Introduction}

The current document is a technical report for the third programming assignment in the M.Sc. course on
\emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc Program}\footnote{https://dws.csd.auth.gr/} of the Aristotle University of Thessaloniki, Greece. The course is taught by Professor Apostolos Papadopoulos~\footnote{https://datalab-old.csd.auth.gr/$\sim$apostol/}. The authors attended the course during their first year of Ph.D. studies at the Institution.

The assignment contains 4 sub-problems and is part of a series, comprising 3 programming assignments on the following topics:
\begin{description}
  \item[Assignment 1] Multi-threading Programming and Inter-Process Communication
  \item[Assignment 2] The Map-Reduce Programming Paradigm
  \item[Assignment 3] Big Data Analytics with Scala and Apache Spark
\end{description}
In this document we focus on Assignment 3 and its 4 sub-problems.
We refer to them as \emph{problems} in the rest of the document for simplicity.
The source code of our solution has been made available at the following public repository in the GitHub platform: \href{https://github.com/Bilpapster/big-data-playground}{\texttt{\small https://github.com/Bilpapster/big-data-playground}}.

\textbf{Roadmap}.
The rest of our work is structured as follows.
We devote one section to each one of the 4 problems.
That means problems 1, 2, 3 and 4 are presented in~\autoref{sec:problem1},~\autoref{sec:problem2},~\autoref{sec:problem3} and~\autoref{sec:problem4} respectively.
For each problem, we first provide the problem statement, as given by the assignment.
Next, we thoroughly present the reasoning and/or methodology we have adopted to approach the problem and devise a solution.
Wherever applicable, we also provide insights about the source code implementation we have developed.
Finally, we conclude our work in~\autoref{sec:conclusion}.
The appendix includes the evaluation results for any issues that necessitated them.

\section{Problem 1: Word Length Analytics}
\label{sec:problem1}
We discuss here the first problem of the assignment.
The main target of the assignment is to get familiar with a simple task leveraging Apache Spark and Scala programming language. This is a WordCount problem's variation.

\subsection{Problem Statement}
Implement an \textbf{Apache Spark} (Scala) program, a variation of the \textbf{word-count} problem, to compute the average length of words that start with a specific letter (a-z). The program should sort the results based on the average length, printing first the letters with higher average length. For example:
\begin{description}
  \item k – $8.2$
  \item a – $5.6$
  \item b – $4.8$
\end{description}
Note that you may opt for preprocessing the input text, e.g., transforming all letters to lowercase or ignoring words that start with a number.

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
We have used the Software Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
The project is compiled and executed directly from the IntelliJ IDEA.

To run the project, open the \texttt{src/main/scala/Task1WordLength.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task1} directory.

\subsubsection{Implementation}
The proposed approach leverages the Apache Spark computing framework to calculate the average word length for each one of the 26 letters in the Latin alphabet. In particular, the input text is read and tokenized into words. As part of a preprocessing step, all letters are transformed to lowercase and all words that start with a number are dropped.

In order to compute the average length for each initial letter, we leverage the \texttt{RDD} API of Apache Spark. More specifically, we use a \texttt{PairRDD} that stores composite key-value pairs in the form \texttt{(letter, (word\_length, 1))}. The key of each pair is the initial letter of the respective word (\texttt{letter}). The value of each pair is a pair itself, having as (false) key the length of the word (\texttt{word\_length}) and as (false) value a unary value (\texttt{1}). We use the term \emph{false} for the key and value of the inner pair, since we do not utilize them as an actual key-value pair. As a matter of fact, we treat the inner pair as a plain tuple of values,.

Subsequently, we group all pairs by key (i.e., initial letter) and compute the following two intermediate results \emph{per initial letter}:
\begin{enumerate}
  \item sum of word lengths that start with the respective letter, by adding up all false keys of the inner pair, and
  \item total number of words that start with the respective letter, by adding up all unary values (false values) of the inner pair.
\end{enumerate}

Having computed these intermediate results for each initial letter, the average length can be computed as the fraction
\begin{displaymath}
  average~word~length_{~letter} = \frac{sum~of~word~lengths_{~letter}}{total~number~of~words_{~letter}},~ \forall letter \in Latin~Alphabet
\end{displaymath}

As a final step, the results are sorted in descending order w.r.t. the average length and saved to a text file. By default, after successful execution of the program, the results can be found under the \texttt{output/task1} directory.

\subsubsection{Evaluation}
The experiments were executed using the dataset \texttt{SherlockHolmes.txt}. The results are listed below in a 10-column format and graphically depicted in~\autoref{img:histogram:word_lengths}.
\begin{multicols}{10}
  \noindent
  c:7.19
  \\ e: 7.11
  \\ q: 7.01
  \\ p: 7.01
  \\ r: 6.87
  \\ d: 6.37
  \\ v: 5.98
  \\ g: 5.93
  \\ s: 5.8
  \\ z: 5.74
  \\ u: 5.61
  \\ j: 5.59
  \\ k: 5.37
  \\ l: 5.34
  \\ f: 5.17
  \\ m: 5.14
  \\ n: 4.81
  \\ b: 4.54
  \\ w: 4.28
  \\ h: 3.81
  \\ a: 3.74
  \\ y: 3.72
  \\ t: 3.64
  \\ i: 3.46
  \\ x: 3.41
  \\ o: 3.01
  \label{multicol:test}
\end{multicols}

\begin{figure}[tb!]
  \centering
  \includegraphics[width=\linewidth]{figures/download}
  \caption{A histogram depicting the average word length (y-axis) for every letter of the Latin alphabet (x-axis). The mean average length is shown as a horizontal red dashed line.}
  \label{img:histogram:word_lengths}
\end{figure}

\section{Problem 2: Movie Analytics}
\label{sec:problem2}
The second problem focuses on producing analytic insights from an IMDB dataset.


\subsection{Problem Statement}
Implement a \textbf{MapReduce} program to perform analytics tasks on an IMDB dataset about movies. The utter goal of your analysis would be to extract useful insights from the available movie data that will assist the IMDB team provide better recommendations for movies, based on their genre and/or country. In particular, the dataset (\texttt{movies.csv}) contains the following fields:
\begin{itemize}
  \item imdbID: unique identifier of the movie in the IMDB database
  \item title: the movie title
  \item year: the year the movie was first released
  \item duration: the duration of the movie
  \item genre(s): the genre or genres in which the movie is classified
  \item premier date: the date of the first showing of the movie
  \item score: the IMDB score of the movie
  \item country/-ies: the country or countries the movie was produced in
\end{itemize}

You are asked to implement Map-Reduce source code in Java programming language for the following analytics tasks:
\begin{itemize}
  \item Calculate the total duration of all movies per country. Note that in case multiple countries are recorded for a movie, the respective duration should be counted for all of them separately.
  \item Calculate the total number of movies per year and genre, having IMDB score over 8. For movies that have more than one genre, the sum should be separate for each genre.
\end{itemize}

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
We have used the Java Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
the repository.
The project is compiled and executed directly from IntelliJ IDEA.

To run the project, open the \texttt{MovieAnalyticsMaster} class in IntelliJ IDEA, and execute the main method.
You will need to specify the following command-line arguments:

\begin{itemize}
  \item \texttt{<input\_path>} specifies the directory containing the input text files, located at \texttt{map-reduce/movieAnalytics/ input}.
  \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be created in the \texttt{out} folder.
\end{itemize}

IntelliJ IDEA will handle the compilation and execution automatically when the main method is run. Make sure to configure the input and output paths as required for your specific run. Note that the command-line arguments must follow the specified order and format. If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

\subsubsection{Implementation}
The proposed approach leverages the MapReduce programming paradigm to compute the analytical insights for the two tasks. \autoref{img:movieAnalyticsSolutionDiagram} depicts the architecture of our solution as a diagram. We opt for executing two separate jobs; one for each task presented by the problem statement.

\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/movieAnalytics}
  \caption{The MapReduce solution architecture for the movie analytics problem. Two separate jobs are executed to solve the two tasks, namely duration per country (green) and movies per year \& genre (yellow). The two tasks also create separate subdirectories for writing the results.}
  \label{img:movieAnalyticsSolutionDiagram}
\end{figure}

\paragraph{Task 1: Duration per country} One map-reduce cycle is enough to handle this task, as depicted in the top part of~\autoref{img:movieAnalyticsSolutionDiagram} in green color. The map function parses the CSV file line by line and extracts the useful fields from each line. In this case, the useful fields are the country (or countries) the movie was produced and the movie duration, i.e., the fourth and ninth fields in the input line respectively. We employ more complex logic to handle cases where there are multiple countries in a single movie. In particular, we parse again the field and tokenize it into the separate countries, producing a key-value pair for each country-duration pair within a movie. The reduce function is, then, trivial; it just adds up the durations (value) per country (key) and outputs the results. The interested reader can refer to the \texttt{CSVProcessor.java} (mapper), \texttt{AnayticsEngine.java} (reducer) and \texttt{MovieAnalyticsMaster.java} (driver) files for the source code implementation of our solution.

\paragraph{Task 2: Movies per year \& genre w.r.t. score constraint} The task is similar to the first one, with the only difference lying in the fields extracted from the input CSV line. In this case, we are interested about the year, the genre (or genres) and score fields, i.e., the third, fifth and seventh fields in the input CSV line. The map function produces key-value pairs in the form \texttt{(composite\_key, 1)}, where the composite key consists of the year and the genre of the respective movie concatenated by an underscore, e.g., \texttt{2024\_action}. We handle multiple genres per movie similarly with the multiple countries in task 1. We employ the same trivial reducer that adds up the values (ones) per (composite) key, as shown in the bottom part of~\autoref{img:movieAnalyticsSolutionDiagram} in yellow color.

\subsubsection{Evaluation} Our solution is tested using the provided IMDB dataset, namely \texttt{movies.csv}. We list the execution results in~\autoref{sec:data2}.

\section{Problem 3: Movie Analytics}
\label{sec:problem3}
The third problem focuses on producing analytic insights into a movies dataset.


\subsection{Problem Statement}
In this problem, you are provided with a dataset that contains information about movies. Each record represents a movie and includes three main columns (attributes), namely \texttt{movieId}, \texttt{title} and \texttt{genres}. An example record of the dataset is the following:

\begin{verbatim}
1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy
\end{verbatim}

Note that a movie can belong to multiple genres (e.g., adventure, fantasy, and so on). The genres are separated by a vertical line (`|') in the respective field. Your Apache Spark (Scala) application should compute the following analytics:
\begin{enumerate}
  \item \textbf{How many movies are there for each genre?} If a movie belongs to multiple genres, it should be counted to all these genres. Sort the results by the name of genre in alphabetical order.
  \item \textbf{How many movies have been filmed per year?} Note that the year a movie was filmed is currently encoded into a composite movie title, e.g., ``\texttt{Toy Story (1995)}''. Show the top 10 years with the most movies filmed within them.
  \item \textbf{Which are the words that appear at least 10 times in the titles of the movies, and what is their total frequency?} You can ignore words that have less than 4 characters. Sort the results, showing first the words with the higher frequency.
\end{enumerate}

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
We have used the Software Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
The project is compiled and executed directly from the IntelliJ IDEA.

To run the project, open the \texttt{src/main/scala/Task3MovieAnalytics.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task3} directory.

\subsubsection{Implementation} We leverage Apache Spark \emph{DataFrames} API to calculate the aforementioned analytical queries. Our proposed approach comprises the following four stages:
\begin{description}
  \item[Stage 1:] Data cleaning and preprocessing
  \item[Stage 2:] Computation of genre analytics (query 1)
  \item[Stage 3:] Computation of year analytics (query 2)
  \item[Stage 4:] Computation of title analytics (query 3)
\end{description}
\autoref{img:movieAnalyticsSolutionDiagram} graphically depicts the four stages, as well as their assembly in a workflow. We elaborate on each stage separately in the rest of the Subsection.

\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/movieAnalytics}
  \caption{The MapReduce solution architecture for the movie analytics problem. Two separate jobs are executed to solve the two tasks, namely duration per country (green) and movies per year \& genre (yellow). The two tasks also create separate subdirectories for writing the results.}
  \label{img:movieAnalyticsSolutionDiagram}
\end{figure}

\paragraph{Stage1: Data Cleaning and preprocessing} In this stage, we address data quality issues that are present in the raw movie data. Missing production year, missing genres and trailing whitespaces in the title string are the detected data quality issues. To address these issues, we explicitly mark the movie year as ``N/A'' (not available) \& movie genre as ``(no genre listed)'' and trim the composite title strings, respectively. Subsequently, we transform the data representation to a more efficient form, by extracting the actual title and year to separate fields (instead of a composite one) and splitting the genres composite string field to an array of genres. Throughout the whole process, we \textbf{use solely the DataFrame API} of Apache Spark, leveraging also the pattern matching functionality of Scala. A small sample of the cleaned and transformed DataFrame shown in~\autoref{img:movieAnalyticsSolutionDiagram} is depicted below:

\begin{verbatim}
                  +-------+--------------------+--------------------+----+
                  |movieId|               title|              genres|year|
                  +-------+--------------------+--------------------+----+
                  |      1|           Toy Story|[Adventure, Anima...|1995|
                  |      2|             Jumanji|[Adventure, Child...|1995|
                  |      3|    Grumpier Old Men|   [Comedy, Romance]|1995|
                  |      4|   Waiting to Exhale|[Comedy, Drama, R...|1995|
                  |      5|Father of the Bri...|            [Comedy]|1995|
                  +-------+--------------------+--------------------+----+
\end{verbatim}

\paragraph{Stage 2: Computation of genre analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie genres. In particular, we use the DataFrame API (\texttt{select()}, \texttt{group\_by()}, etc.) to calculate the number of movies that belong to each genre. An implementation detail of our solution lies in handling the cases where a movie belongs to multiple genres. To ensure the integrity of the computed analytics, we opt for using the \texttt{explode()} function of the DataFrame API, to transform the array of genres to separate genre items. The analytics are, then, computed on top of this extra transformation, enabling us to include \textbf{a movie into the calculations of all the genres it belongs}.

\paragraph{Stage 3: Computation of year analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie production years. Similarly to Stage 2, we leverage exclusively the DataFrame API to calculate the total number of movies filmed each year. In this case, the \texttt{group\_by} operation is more straightforward, since the cleaned and transformed DataFrame contains a dedicated column for the movie production year.

\paragraph{Stage 4: Computation of title analytics} Similarly to Stage 2, we use operations available from the DataFrame API to calculate the most frequently used words in the movie titles. We, leverage the \texttt{split(" ")} and \texttt{explode()} functions to transform titles, first, into an array of words and, then, into separate words that are finally grouped and aggregated. We remove words that have less than 4 characters and fundamental stopwords, such as ``with'', or ``from'', in order to obtain analytical insights of high quality.

\subsubsection{Evaluation} Our solution is tested using the raw movie data, namely \texttt{movies.csv}. We list the execution results in~\autoref{sec:data3}.

\paragraph{Stage 2: Computation of genre analytics}
We list here the execution results for the genre analytics query in a 6-column format.
\begin{multicols}{6}
  \noindent
  Action: 7348
  \\ Adventure: 4145
  \\ Animation: 2929
  \\ Children: 2935
  \\ Comedy: 16870
  \\ Crime: 5319
  \\ Documentary: 5605
  \\ Drama: 25606
  \\ Fantasy: 2731
  \\ Film-Noir: 353
  \\ Horror: 5989
  \\ IMAX: 195
  \\ Musical: 1054
  \\ Mystery: 2925
  \\ Romance: 7719
  \\ Sci-Fi: 3595
  \\ Thriller: 8654
  \\ War: 1874
  \\ Western: 1399
  \\ N/A: 5062
\end{multicols}

\paragraph{Stage 3: Computation of year analytics}
We list here the execution results for the year analytics query in a 3-column format, i.e., the top 10 years with the most movies. We observe that these years span the time period between 2009 and 2018, excluding 2015, indicating a heavy film production process in this decade. \autoref{img:movies_per_year} depicts the distribution of movies per year for all years in chronological order. Our findings indicate a clear long-tail distribution of movies over the years, with a burst of new movies over the last decade.
\begin{multicols}{3}
  \centering
  \noindent
  2015: 2512 movies
  \\ 2016: 2488 movies
  \\ 2014: 2406 movies
  \\ 2017: 2373 movies
  \\ 2013: 2173 movies
  \\ 2018: 2032 movies
  \\ 2012: 1978 movies
  \\ 2011: 1838 movies
  \\ 2009: 1724 movies
  \\ 2010: 1691 movies
\end{multicols}

\begin{figure}[tb!]
  \centering
  \includegraphics[width=\linewidth]{figures/movies per year}
  \caption{The distribution of the total number of movies per year in the dataset used for evaluating our solution.}
  \label{img:movies_per_year} 
\end{figure}

\paragraph{Stage 4: Computation of title analytics}
We list in~\autoref{sec:data3} the execution results for the title analytics query.Also, ~\autoref{img:word_cloud} depicts a word cloud with the most frequently used words in movie titles, found in the dataset used for evaluating our solution. Note that the bigger a word is, the more frequently it appears in movie titles.
\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/word cloud}
  \caption{A word cloud depicting the most frequent words appearing in the titles of the movies in the dataset examined. The bigger a word is, the more frequently it appears in the titles of the movies.}
  \label{img:word_cloud}
\end{figure}

\section{Problem 4: Probabilistic graph}
\label{sec:problem4}
We discuss here the fourth problem of the assignment.
The main target of the assignment is to get acquainted with performing various MapReduce phases on a pipeline frame.

\subsection{Problem Statement}
We are given an input file in text format where each line contains a connection between two vertices of a network and a
probability value.
For each edge \( e \), there is a probability value \( p(e) \) which indicates the probability that the two vertices are
connected by the edge.
Obviously, the values of \( p(e) \) range between 0 and 1.
The values in each line are separated by a space.
Consider the network shown in the previous figure.
The edge connecting vertices 4 and 5 has a probability of
0.8, the edge connecting vertices 2 and 3 has a probability of 0.2, etc.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.25\linewidth]{figures/graph}
\end{figure}

The file corresponding to this graph would be:

\[
  1 \ 2 \ 0.6, \\
  1 \ 3 \ 0.9, \\
  2 \ 3 \ 0.2, \\
  3 \ 4 \ 0.75, \\
  2 \ 5 \ 0.2, \\
  4 \ 5 \ 0.8,
\]

The edges are generally stored in random order in the file, so we cannot assume they have a specific arrangement.

The following tasks are requested:

\begin{enumerate}
  \item Write a Java program that computes the average degree for all the vertices.
  \item The average degree is defined as the sum of the probabilities of the edges that fall on a vertex.
  \item For example, in the previous diagram, the average degree of vertex 3 is \( 0.9 + 0.2 + 0.75 = 1.85 \).
  \item Before performing this calculation, you should ignore all edges with a probability less than a threshold \( T \)
        , which should be passed as a parameter to the main function.
  \item Modify the code from task 1 so that, at the end, only the vertices with an average degree greater than the
        average of the degrees of all the vertices are displayed in the output.

\end{enumerate}

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
We have used the Java Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
the repository.
The project is compiled and executed directly from IntelliJ IDEA.

To run the project, open the \texttt{GraphMaster} class in IntelliJ IDEA, and execute the main method.
You will need to specify the following command-line arguments:

\begin{itemize}
  \item \texttt{<input\_path>} specifies the directory containing the input text files, located at
        \texttt{map-reduce/ probabilisticGraph /input}.
  \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be
        created in the \texttt{out} folder.
  \item \texttt{<T>} is the minimum edge-degree threshold for vertices to be included in the results.
\end{itemize}

IntelliJ IDEA will handle the compilation and execution automatically when the main method is run.
Make sure to configure the input and output paths as required for your specific run.

Note that the command-line arguments must follow the specified order and format.
If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

\subsubsection{Implementation}
The problem at hand involves processing a probabilistic graph, where each edge connects two vertices with a probability
value.
The task is to compute the average degree for each vertex, considering only edges whose probability exceeds a given
threshold \( T \).
The methodology proposed here utilizes a distributed MapReduce framework, implemented using Hadoop, to efficiently
process and analyze large-scale graph data.
The approach is divided into three main phases, each handled by separate MapReduce jobs, with intermediate results
passed between the phases.

The \texttt{GraphMaster} class serves as the orchestrator of the entire MapReduce process.
It manages the execution of the three phases, invoking the appropriate MapReduce jobs in sequence.
The main steps executed by \texttt{GraphMaster} are:

\begin{enumerate}
  \item It retrieves the command-line arguments, including the input and output directories as well as the threshold
        value \( T \), which determines which edges to consider in the graph.
  \item It initiates the first MapReduce job to compute the degree of each vertex in the graph.
        The input consists of edge data, and the output is a summation of edge probabilities for each vertex.
  \item After the first job completes, the \texttt{GraphMaster} starts the second MapReduce job to calculate the mean
        degree of the graph, using the results from the first job.
  \item Once the mean degree is computed, \texttt{GraphMaster} starts the third MapReduce job, which filters out the
        vertices with degrees lower than the mean degree.
  \item It cleans up intermediate directories after each phase and moves the final output to the desired location.
\end{enumerate}

\texttt{GraphMaster} coordinates these steps by configuring and executing the MapReduce jobs, ensuring that each phase
depends on the results of the previous one. \\

The entire process consists of three MapReduce jobs, executed sequentially:

\begin{enumerate}
  \item \textbf{Phase 1:} The first job computes the degree of each vertex by summing the probabilities of the edges
        connected to it.
        It produces intermediate results for each vertex.
  \item \textbf{Phase 2:} The second job calculates the mean degree by summing the degrees from the first phase and
        dividing by the total number of vertices.
  \item \textbf{Phase 3:} The third job filters out the vertices with a degree lower than the mean degree and produces
        the final filtered result.
\end{enumerate}

After each job completes, intermediate data is written to disk and passed as input to the subsequent job.
The final output consists of the vertices whose degree exceeds or equals the mean degree. \\

\textbf{Phase 1: Calculating Vertex Degree}
The first MapReduce job is responsible for calculating the degree of each vertex in the graph, where the degree is
defined as the sum of the probabilities of the edges connected to that vertex.
The degree of each vertex is computed by the \texttt{GraphMapper} and \texttt{GraphReducer} classes.

The \texttt{GraphMapper} class reads each edge from the input data, which consists of lines containing two vertices
and the associated probability value.
It splits each line into two components (the two vertices), and for each vertex, it emits the corresponding probability
value as the output.
The mapper also checks if the probability value exceeds the threshold \( T \), which is provided as a configuration
parameter.
If the probability is greater than or equal to \( T \), it emits the vertex along with the corresponding probability.

The \texttt{GraphReducer} class receives the vertex and associated probability values emitted by the mapper.
For each vertex, it sums up all the probabilities of the edges connected to that vertex, and emits the pair as output.

\textbf{Phase 2: Calculating the Mean Degree}
The second phase of the approach calculates the mean degree of all vertices.
This is done by summing the degree values from the previous phase and dividing by the total number of vertices.

The \texttt{MeanMapper} class receives the degree values from the output of the first MapReduce job.
It emits two key-value pairs for each input line: a count of the number of vertices and the sum of the degree values.
These are emitted with fixed keys (\texttt{"count"} and \texttt{"sum"}) so that they can be aggregated in the reducer.

The \texttt{MeanReducer} class processes the count and sum values emitted by the mapper.
It calculates the total sum and count of vertices and then writes these values as output.
The mean degree is calculated by dividing the sum by the count, and the result is stored for use in the next phase.

\textbf{Phase 3: Filtering Vertices by Mean Degree}
In the third phase, the vertices whose degree is greater than or equal to the mean degree calculated in Phase 2 are
selected.
This phase uses the results from Phase 1 and Phase 2, applying the filtering criteria to retain only those vertices with
a degree greater than or equal to the mean.

The \texttt{FilterMapper} class takes the degree values emitted by the first reducer and writes them as key-value pairs.
Each key is a vertex, and the value is the degree of that vertex.
The mapper does not perform any filtering; instead, it simply passes the values along to the reducer.

The \texttt{FilterReducer} class filters out the vertices whose degree is less than the mean degree.
It retrieves the mean degree from the configuration, and for each vertex, it compares the degree value to the mean.
If the degree is greater than or equal to the mean, it emits the vertex along with its degree.
This final output contains only the vertices that satisfy the filtering criteria.

\subsubsection{Evaluation}
The experiments were executed using the dataset `collins.txt` with a parameter setting of \( T = 0.8 \).

To provide a comprehensive overview, the results of the experiment, including the detailed values derived from the
dataset, are presented in~\autoref{sec:data4} in a 2-column format.



\section{Conclusion}
\label{sec:conclusion}
In this document we have presented our solutions and rationale for solving the second assignment of the M.Sc.
course on \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc. Program}. For each one of the four problems of the assignment, we have presented their statement, as well as the solution approach we have adopted. All execution results with the provided datasets can be found in the appendices of our work.

\newpage
\appendix

\section{Problem 2 execution results}
\label{sec:data2}
\input{appendix2}

\section{Problem 3 execution results}
\label{sec:data3}
\input{appendix3}

\subsection{Problem 4 execution results}
\label{sec:data4}
\input{appendix4}


\end{document}
\endinput

