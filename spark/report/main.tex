\documentclass[acmlarge]{acmart}



\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
\settopmatter{printacmref=false}

\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{geometry}
\newcommand{\todo}{{\color{red}\textbf{TODO} }}
\newcommand{\disease}{{\small \texttt{DISEASE}} }
\newcommand{\hospital}{{\small \texttt{HOSPITAL}} }
\newcommand{\storage}{{\small \texttt{STORAGE}} }

% christos 2, 4
% vasilis 1, 3

\usepackage{amsmath}
\usepackage{subfig}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocodex}
\newcommand{\algorithmautorefname}{Algorithm}

\begin{document}

  \title{Big Data Analytics with Scala and Apache Spark}
  \subtitle{M.Sc. course on ``Technologies for Big Data Analysis'' - Assignment 3}

  \author{Christos Balaktsis (506)}
  \email{balaktsis@csd.auth.gr}
  \author{Vasileios Papastergios (505)}
  \email{papster@csd.auth.gr}
  \affiliation{
    \institution{Aristotle University}
    \city{Thessaloniki}
    \country{Greece}
  }

  \renewcommand{\shortauthors}{C. Balaktsis and V. Papastergios}
  \maketitle

  \section{Introduction}

  The current document is a technical report for the third programming assignment in the M.Sc. course on
  \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc Program}\footnote{https://dws.csd.auth.gr/} of the Aristotle University of Thessaloniki, Greece. The course is taught by Professor Apostolos Papadopoulos~\footnote{https://datalab-old.csd.auth.gr/$\sim$apostol/}. The authors attended the course during their first year of Ph.D. studies at the Institution.

  The assignment contains 4 sub-problems and is part of a series, comprising 3 programming assignments on the following topics:
  \begin{description}
    \item[Assignment 1] Multi-threading Programming and Inter-Process Communication
    \item[Assignment 2] The Map-Reduce Programming Paradigm
    \item[Assignment 3] Big Data Analytics with Scala and Apache Spark
  \end{description}
  In this document we focus on Assignment 3 and its 4 sub-problems.
  We refer to them as \emph{problems} in the rest of the document for simplicity.
  The source code of our solution has been made available at the following public repository in the GitHub platform: \href{https://github.com/Bilpapster/big-data-playground}{\texttt{\small https://github.com/Bilpapster/big-data-playground}}.

  \textbf{Roadmap}.
  The rest of our work is structured as follows.
  We devote one section to each one of the 4 problems.
  That means problems 1, 2, 3 and 4 are presented in~\autoref{sec:problem1},~\autoref{sec:problem2},~\autoref{sec:problem3} and~\autoref{sec:problem4} respectively.
  For each problem, we first provide the problem statement, as given by the assignment.
  Next, we thoroughly present the reasoning and/or methodology we have adopted to approach the problem and devise a solution.
  Wherever applicable, we also provide insights about the source code implementation we have developed.
  Finally, we conclude our work in~\autoref{sec:conclusion}.
  The appendix includes the evaluation results for any issues that necessitated them.

  \section{Problem 1: Word Length Analytics}
  \label{sec:problem1}
  We discuss here the first problem of the assignment.
  The main target of the assignment is to get familiar with a simple task leveraging Apache Spark and Scala programming language. This is a WordCount problem's variation.

  \subsection{Problem Statement}
  Implement an \textbf{Apache Spark} (Scala) program, a variation of the \textbf{word-count} problem, to compute the average length of words that start with a specific letter (a-z). The program should sort the results based on the average length, printing first the letters with higher average length. For example:
  \begin{description}
    \item k – $8.2$
    \item a – $5.6$
    \item b – $4.8$
  \end{description}
  Note that you may opt for preprocessing the input text, e.g., transforming all letters to lowercase or ignoring words that start with a number.

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task1WordLength.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task1} directory.

  \subsubsection{Implementation}
  The proposed approach leverages the Apache Spark computing framework to calculate the average word length for each one of the 26 letters in the Latin alphabet. In particular, the input text is read and tokenized into words. As part of a preprocessing step, all letters are transformed to lowercase and all words that start with a number are dropped.

  In order to compute the average length for each initial letter, we leverage the \texttt{RDD} API of Apache Spark. More specifically, we use a \texttt{PairRDD} that stores composite key-value pairs in the form \texttt{(letter, (word\_length, 1))}. The key of each pair is the initial letter of the respective word (\texttt{letter}). The value of each pair is a pair itself, having as (false) key the length of the word (\texttt{word\_length}) and as (false) value a unary value (\texttt{1}). We use the term \emph{false} for the key and value of the inner pair, since we do not utilize them as an actual key-value pair; we treat this pair as a plain tuple of values instead.

  Subsequently, we group all pairs by key (i.e., initial letter) and compute the following two intermediate results \emph{per initial letter}:
  \begin{enumerate}
    \item sum of word lengths that start with the respective letter, by adding up all false keys of the inner pair, and
    \item total number of words that start with the respective letter, by adding up all unary values (false values) of the inner pair.
  \end{enumerate}

  Having computed these intermediate results for each initial letter, the average length can be computed as the fraction
  \begin{displaymath}
    average~word~length_{~letter} = \frac{sum~of~word~lengths_{~letter}}{total~number~of~words_{~letter}},~ \forall letter \in Latin~Alphabet
  \end{displaymath}

  As a final step, the results are sorted in descending order w.r.t. the average length and saved to a text file. By default, after successful execution of the program, the results can be found under the \texttt{output/task1} directory.

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/download}
    \caption{A histogram depicting the average word length (y-axis) for every letter of the Latin alphabet (x-axis). The mean average length is shown as a horizontal red dashed line.}
    \label{img:histogram:word_lengths}
  \end{figure}
  

  \subsubsection{Evaluation}
  The experiments were executed using the dataset \texttt{SherlockHolmes.txt}. The results are listed below in a 10-column format and graphically depicted in~\autoref{img:histogram:word_lengths}.

  \begin{multicols}{10}
    \noindent
    c:7.19
    \\ e: 7.11
    \\ q: 7.01
    \\ p: 7.01
    \\ r: 6.87
    \\ d: 6.37
    \\ v: 5.98
    \\ g: 5.93
    \\ s: 5.8
    \\ z: 5.74
    \\ u: 5.61
    \\ j: 5.59
    \\ k: 5.37
    \\ l: 5.34
    \\ f: 5.17
    \\ m: 5.14
    \\ n: 4.81
    \\ b: 4.54
    \\ w: 4.28
    \\ h: 3.81
    \\ a: 3.74
    \\ y: 3.72
    \\ t: 3.64
    \\ i: 3.46
    \\ x: 3.41
    \\ o: 3.01
    \label{multicol:test}
  \end{multicols}

  \newpage
  \section{Problem 2: Airline Tweets analytics}
  \label{sec:problem2}
  The second problem focuses on producing analytic insights from an Twitter feedback post for airline services.


  \subsection{Problem Statement}
  In this task, we take on the role of a data analyst aiming to help airlines improve the quality of their services. Our data source is a CSV file (each row corresponds to a tweet) containing Twitter comments about airline services, with the following format:

  \begin{itemize}
    \item \texttt{tweet\_id}
    \item \texttt{airline\_sentiment}
    \item \texttt{airline\_sentiment\_confidence}
    \item \texttt{negativereason}
    \item \texttt{negativereason\_confidence}
    \item \texttt{airline}
    \item \texttt{name}
    \item \texttt{text}
    \item \texttt{tweet\_created}
    \item \texttt{user\_timezone}
  \end{itemize}

  You are required to create a program using Spark DataFrames to answer the following questions:

  \begin{enumerate}
    \item \textbf{What are the 5 words in the tweet text that appear most frequently for each \texttt{airline\_sentiment} category: positive, negative, and neutral?}
    \item \textbf{What is the main reason for complaint (\texttt{negativereason}) for each airline, i.e., the reason associated with the most tweets?} Consider only tweets with \texttt{negativereason\_confidence > 0.5}.
  \end{enumerate}

  In the implementation, you should ignore punctuation marks, and consider all words to be in lowercase for the related processing.

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task2AirlineTweets.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task2} directory.

  \subsubsection{Implementation}
  The implementation utilizes Apache Spark for distributed processing, ensuring scalability for large datasets. The process is divided into two main tasks, described in detail below.
  \\

  \textit{Task 1: Extracting Top Words by Sentiment}
  \\

  \textbf{Preprocessing:}
  \begin{enumerate}
    \item Text was converted to lowercase to ensure case insensitivity.
    \item Non-alphabetic characters were removed using regular expressions, eliminating punctuation and special symbols.
  \end{enumerate}

  \textbf{Word Frequency Analysis:}
  \begin{enumerate}
    \item Tweets were filtered based on their sentiment (\texttt{airline\_sentiment} = positive, negative, or neutral).
    \item Tokenization was performed to split the cleaned text into individual words.
    \item A word frequency distribution was computed by grouping words and counting their occurrences.
    \item The top 5 most frequent words for each sentiment category were extracted and ranked.
  \end{enumerate}

  \textbf{Output:}
  Results were saved in separate directories (\texttt{output/task2/\{sentiment\}-top-words}) for each sentiment category as CSV files.
  \\

  \textit{Task 2: Identifying Main Complaint Reasons}
  \\

  \textbf{Filtering:}
  \begin{enumerate}
    \item Tweets with a non-null \texttt{negativereason} field were selected.
    \item Only tweets with \texttt{negativereason\_confidence} > 0.5 were considered to ensure reliable complaint reasons.
  \end{enumerate}

  \textbf{Grouping and Ranking:}
  \begin{enumerate}
    \item Tweets were grouped by \texttt{airline} and \texttt{negativereason}.
    \item The frequency of each complaint reason was calculated for each airline.
    \item A ranking function (\texttt{row\_number}) was applied within each airline's group to identify the most frequent complaint reason.
  \end{enumerate}

  \textbf{Output:}
  The results, including the airline, top complaint reason, and its frequency, were saved in a CSV file (\texttt{output/task2/top-complaints}).

  \paragraph{Implementation Details}
  The implementation uses the following key components of Apache Spark:
  \begin{itemize}
    \item \textbf{SparkSession:} Initializes the Spark application and provides the entry point for DataFrame operations.
    \item \textbf{DataFrame API:} Used for transformations, including filtering, grouping, and ranking.
    \item \textbf{Regular Expressions:} Applied to clean the text data by removing non-alphabetic characters.
    \item \textbf{Window Functions:} Enabled ranking of complaint reasons for each airline.
  \end{itemize}



  \subsubsection{Evaluation} Our solution is tested using the provided tweets dataset, namely \texttt{tweets.csv}. The results for each analytical query are presented below.

  \begin{table}[h!]
    \centering
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        the & 903 \\ \hline
        to & 880 \\ \hline
        you & 811 \\ \hline
        for & 628 \\ \hline
        thanks & 587 \\ \hline
      \end{tabular}
      \caption{Top-5 positive words}
    \end{minipage} \hspace{0.3cm}
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        to & 1573 \\ \hline
        i & 1126 \\ \hline
        the & 927 \\ \hline
        a & 774 \\ \hline
        united & 700 \\ \hline
      \end{tabular}
      \caption{Top-5 neutral words}
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Word} & \textbf{Count} \\ \hline
        to & 5686 \\ \hline
        the & 3914 \\ \hline
        i & 3396 \\ \hline
        a & 3033 \\ \hline
        flight &2763 \\ \hline
      \end{tabular}
      \caption{Top-5 negative words}
    \end{minipage}
  \end{table}

  \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Airline} & \textbf{Reason} & \textbf{Count} \\ \hline
      American & Customer Service Issue & 654  \\ \hline
      Delta & Late Flight & 228 \\ \hline
      Southwest & Customer Service Issue & 323 \\ \hline
      US Airways & Customer Service Issue & 698 \\ \hline
      United & Customer Service Issue & 545 \\ \hline
      Virgin America & Customer Service Issue & 51 \\ \hline
    \end{tabular}
    \caption{Top complaints for each Airline}
  \end{table}


  \section{Problem 3: Movie Analytics}
  \label{sec:problem3}
  The third problem focuses on producing analytic insights into a movies dataset.


  \subsection{Problem Statement}
  In this problem, you are provided with a dataset that contains information about movies. Each record represents a movie and includes three main columns (attributes), namely \texttt{movieId}, \texttt{title} and \texttt{genres}. An example record of the dataset is the following:

  \begin{verbatim}
1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy
  \end{verbatim}

  Note that a movie can belong to multiple genres (e.g., adventure, fantasy, and so on). The genres are separated by a vertical line (`|') in the respective field. Your Apache Spark (Scala) application should compute the following analytics:
  \begin{enumerate}
    \item \textbf{How many movies are there for each genre?} If a movie belongs to multiple genres, it should be counted to all these genres. Sort the results by the name of genre in alphabetical order.
    \item \textbf{How many movies have been filmed per year?} Note that the year a movie was filmed is currently encoded into a composite movie title, e.g., ``\texttt{Toy Story (1995)}''. Show the top 10 years with the most movies filmed within them.
    \item \textbf{Which are the words that appear at least 10 times in the titles of the movies, and what is their total frequency?} You can ignore words that have less than 4 characters. Sort the results, showing first the words with the higher frequency.
  \end{enumerate}

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task3MovieAnalytics.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task3} directory.

  \subsubsection{Implementation} We leverage Apache Spark \emph{DataFrames} API to calculate the aforementioned analytical queries. Our proposed approach comprises the following four stages:
  \begin{description}
    \item[Stage 1:] Data cleaning and preprocessing
    \item[Stage 2:] Computation of genre analytics (query 1)
    \item[Stage 3:] Computation of year analytics (query 2)
    \item[Stage 4:] Computation of title analytics (query 3)
  \end{description}
  \autoref{img:movieAnalyticsSolutionDiagram} graphically depicts the four stages, as well as their assembly in a workflow. We elaborate on each stage separately in the rest of the Subsection.

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/movieAnalytics}
    \caption{The four stages of our proposed approach to compute the movie analytics, assembled in a workflow.}
    \label{img:movieAnalyticsSolutionDiagram}
  \end{figure}

  \paragraph{Stage1: Data Cleaning and preprocessing} In this stage, we address data quality issues that are present in the raw movie data. Missing production year, missing genres and trailing whitespaces in the title string are the detected data quality issues. To address these issues, we explicitly mark the movie year as ``N/A'' (not available) \& movie genre as ``(no genre listed)'' and trim the composite title strings, respectively. Subsequently, we transform the data representation to a more efficient form, by extracting the actual title and year to separate fields (instead of a composite one) and splitting the genres composite string field to an array of genres. Throughout the whole process, we \textbf{use solely the DataFrame API} of Apache Spark, leveraging also the pattern matching functionality of Scala. A small sample of the cleaned and transformed DataFrame shown in~\autoref{img:movieAnalyticsSolutionDiagram} is depicted below:

  \begin{verbatim}
                  +-------+--------------------+--------------------+----+
                  |movieId|               title|              genres|year|
                  +-------+--------------------+--------------------+----+
                  |      1|           Toy Story|[Adventure, Anima...|1995|
                  |      2|             Jumanji|[Adventure, Child...|1995|
                  |      3|    Grumpier Old Men|   [Comedy, Romance]|1995|
                  |      4|   Waiting to Exhale|[Comedy, Drama, R...|1995|
                  |      5|Father of the Bri...|            [Comedy]|1995|
                  +-------+--------------------+--------------------+----+
  \end{verbatim}

  \paragraph{Stage 2: Computation of genre analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie genres. In particular, we use the DataFrame API (\texttt{select()}, \texttt{group\_by()}, etc.) to calculate the number of movies that belong to each genre. An implementation detail of our solution lies in handling the cases where a movie belongs to multiple genres. To ensure the integrity of the computed analytics, we opt for using the \texttt{explode()} function of the DataFrame API, to transform the array of genres to separate genre items. The analytics are, then, computed on top of this extra transformation, enabling us to \textbf{include a movie into the calculations of all the genres it belongs}.

  \paragraph{Stage 3: Computation of year analytics} In this stage, we use the cleaned and transformed data to compute analytical insights into the movie production years. Similarly to Stage 2, we leverage exclusively the DataFrame API to calculate the total number of movies filmed each year. In this case, the \texttt{group\_by} operation is more straightforward, since the cleaned and transformed DataFrame contains a dedicated column for the movie production year.

  \paragraph{Stage 4: Computation of title analytics} Similarly to Stage 2, we use operations available from the DataFrame API to calculate the most frequently used words in the movie titles. We, leverage the \texttt{split(" ")} and \texttt{explode()} functions to transform titles, first, into an array of words and, then, into separate words that are finally grouped and aggregated. We remove words that have less than 4 characters and fundamental stopwords, such as ``with'', or ``from'', in order to obtain analytical insights of high quality.

  \subsubsection{Evaluation} Our solution is tested using the raw movie data, namely \texttt{movies.csv}. We list the execution results below, if of appropriate length, else in~\autoref{sec:data3}.

  \paragraph{Stage 2: Computation of genre analytics}
  We list here the execution results for the genre analytics query in a 6-column format.
  \begin{multicols}{6}
    \noindent
    Action: 7348
    \\ Adventure: 4145
    \\ Animation: 2929
    \\ Children: 2935
    \\ Comedy: 16870
    \\ Crime: 5319
    \\ Documentary: 5605
    \\ Drama: 25606
    \\ Fantasy: 2731
    \\ Film-Noir: 353
    \\ Horror: 5989
    \\ IMAX: 195
    \\ Musical: 1054
    \\ Mystery: 2925
    \\ Romance: 7719
    \\ Sci-Fi: 3595
    \\ Thriller: 8654
    \\ War: 1874
    \\ Western: 1399
    \\ N/A: 5062
  \end{multicols}

  \paragraph{Stage 3: Computation of year analytics}
  We list here the execution results for the year analytics query in a 3-column format, i.e., the top 10 years with the most movies. We observe that these years span the time period between 2009 and 2018, excluding 2015, indicating a heavy film production process in this decade. \autoref{img:movies_per_year} depicts the distribution of movies per year for all years in chronological order. Our findings indicate a clear long-tail distribution of movies over the years, with a burst of new movies over the last decade.
  \begin{multicols}{3}
    \centering
    \noindent
    2015: 2512 movies
    \\ 2016: 2488 movies
    \\ 2014: 2406 movies
    \\ 2017: 2373 movies
    \\ 2013: 2173 movies
    \\ 2018: 2032 movies
    \\ 2012: 1978 movies
    \\ 2011: 1838 movies
    \\ 2009: 1724 movies
    \\ 2010: 1691 movies
  \end{multicols}

  \begin{figure}[tb!]
    \centering
    \includegraphics[width=\linewidth]{figures/movies per year}
    \caption{The distribution of the total number of movies per year in the dataset used for evaluating our solution.}
    \label{img:movies_per_year}
  \end{figure}

  \paragraph{Stage 4: Computation of title analytics}
  We list in~\autoref{sec:data3} the execution results for the title analytics query. Also, ~\autoref{img:word_cloud} depicts a word cloud with the most frequently used words in movie titles, found in the dataset used for evaluating our solution. Note that the bigger a word is, the more frequently it appears in movie titles.
  \begin{figure}[tb!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/word cloud}
    \caption{A word cloud depicting the most frequent words appearing in the titles of the movies in the dataset examined. The bigger a word is, the more frequently it appears in the titles of the movies.}
    \label{img:word_cloud}
  \end{figure}

  \section{Problem 4: Probabilistic graph}
  \label{sec:problem4}
  We discuss here the fourth problem of the assignment.
  The main target of the assignment is to get acquainted with performing various MapReduce phases on a pipeline frame.

  \subsection{Problem Statement}
  Given a directed graph consisting of nodes and directed edges, stored in the edge-list format in a txt or csv file, you are requested to do the following tasks:
  \begin{itemize}
    \item Implement a program, utilizing the RDD Spark's framework, that for each node of the graph, it will calculate the number of incoming and the number of outgoing edges. The program should print out the 10 nodes with the most incoming and outgoing edges, respectively.
    \item Modify the previous implementation, so as not to discriminate the direction of the edge, so that the degree of a node is calculated by the number of edges lying on it. The program should print the number of nodes with degree greater than or equal to the mean degree.
  \end{itemize}
  Use the edge-list (txt) file provided in \url{http://snap.stanford.edu/data/web-Stanford.txt.gz}.
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.25\linewidth]{figures/graph}
  \end{figure}

  \subsection{Proposed approach}
  \subsubsection{Setting}
  Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
  We have used the Software Development Kit (JDK) version 11.0.11.
  The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
  The project is compiled and executed directly from the IntelliJ IDEA.

  To run the project, open the \texttt{src/main/scala/Task4GraphEdgeCounts.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task4} directory.


  \subsubsection{Implementation}
  The task involves working with a directed graph stored in an edge-list format, where each edge is represented as a tuple of source and destination nodes. The goal is to develop a solution using Apache Spark's Resilient Distributed Datasets (RDDs) framework to calculate and analyze edge counts for the nodes in the graph. The approach consists of the following key steps:

  \textbf{Preprocessing.}
  First, the graph data, stored in a text or CSV file, is read into an RDD. The RDD is filtered to remove any comments (lines starting with \texttt{\#}), ensuring that only valid edge data is processed.

  For each edge in the graph, the source node is identified, and a tuple \texttt{(src, 1)} is created for each source node. These tuples (\texttt{outgoingCounts}) are then aggregated by the \texttt{reduceByKey} function to calculate the total number of outgoing edges for each node. The result is an RDD of node and outgoing edge count pairs.

  Similarly, for each edge, the destination node is identified, and a tuple \texttt{(dest, 1)} is created for each destination node. The \texttt{reduceByKey} function is again used to calculate the total number of incoming edges for each node,    resulting in an RDD of node and incoming edge count pairs (\texttt{incomingCounts}).

  \textbf{Top 10 nodes}
  To identify the top 10 nodes with the most incoming and outgoing edges, the \texttt{top} function is used, which sorts the nodes in descending order based on the edge counts. These top nodes are saved into separate output files: one for the nodes with the most incoming edges and another for the nodes with the most outgoing edges.

  \textbf{Degree-based filtering}
  In the modified implementation, the direction of the edges is ignored to calculate the degree of each node. The degree is calculated as the sum of the incoming and outgoing edge counts for each node. This is achieved using a \texttt{fullOuterJoin} between the RDDs of incoming and outgoing edge counts, followed by a transformation that sums the values for each node. In cases where a node only has outgoing or incoming edges but not both, the missing values are treated as 0.

  The mean degree of the graph is then calculated by summing the degrees of all nodes and dividing by the total number of nodes. This value is used to filter the nodes with a degree greater than or equal to the mean degree.

  The filtered nodes are saved to an output file, and the total count of such nodes is printed. This provides insight into how many nodes in the graph have a degree that meets or exceeds the average.

  The final output consists of the following:
  \begin{itemize}
    \item The 10 nodes with the most incoming edges.
    \item The 10 nodes with the most outgoing edges.
    \item The number of nodes with a degree greater than or equal to the mean degree, along with the corresponding list of nodes.
  \end{itemize}

  \subsubsection{Evaluation} Our solution is tested using the provided edge-list Stanford dataset, namely \texttt{web-Stanford.txt}. The results are presented below.

  \begin{table}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Node} & \textbf{Incoming} \\ \hline
        226411 & 38606\\ \hline
        234704 & 21920\\ \hline
        105607 & 19457\\ \hline
        241454 & 19377\\ \hline
        167295  & 19003\\ \hline
        198090 & 18975\\ \hline
        81435 & 18970\\ \hline
        214128 & 18967\\ \hline
        38342 & 18958\\ \hline
        245659 & 18935\\ \hline
      \end{tabular}
      \caption{Top-10 nodes with most incoming edges}
    \end{minipage} \hspace{0.3cm}
    \begin{minipage}{0.45\textwidth}
      \centering
      \begin{tabular}{|c|c|}
        \hline
        \textbf{Node} & \textbf{Incoming} \\ \hline
        82409 & 255\\ \hline
        82868 & 247\\ \hline
        188978 & 247\\ \hline
        16984 & 247\\ \hline
        86290 & 247\\ \hline
        180611 & 247\\ \hline
        10699 & 245\\ \hline
        121634 & 245\\ \hline
        176419 & 244\\ \hline
        255711 & 244\\ \hline
      \end{tabular}
      \caption{Top-10 nodes with most outgoing edges}
    \end{minipage}
  \end{table}

  \begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Airline} & \textbf{Reason} & \textbf{Count} \\ \hline
      American & Customer Service Issue & 654  \\ \hline
      Delta & Late Flight & 228 \\ \hline
      Southwest & Customer Service Issue & 323 \\ \hline
      US Airways & Customer Service Issue & 698 \\ \hline
      United & Customer Service Issue & 545 \\ \hline
      Virgin America & Customer Service Issue & 51 \\ \hline
    \end{tabular}
    \caption{Top complaints for each Airline}
  \end{table}

  The number of nodes with degree greater than or equal to the average degree (16.41) is \textbf{54933}.

  \section{Conclusion}
  \label{sec:conclusion}
  In this document we have presented our solutions and rationale for solving the second assignment of the M.Sc.
  course on \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc. Program}. For each one of the four problems of the assignment, we have presented their statement, as well as the solution approach we have adopted. All execution results with the provided datasets can be found in the appendices of our work.

  \newpage
  \appendix

  \section{Problem 3 execution results}
  \label{sec:data3}
  \input{appendix3}

\end{document}
\endinput

