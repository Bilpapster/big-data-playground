\documentclass[acmlarge]{acmart}



\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
  \settopmatter{printacmref=false}

\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\textit{#1}}
\usepackage{xcolor}
\usepackage{multicol}
\usepackage{geometry}
\newcommand{\todo}{{\color{red}\textbf{TODO} }}
\newcommand{\disease}{{\small \texttt{DISEASE}} }
\newcommand{\hospital}{{\small \texttt{HOSPITAL}} }
\newcommand{\storage}{{\small \texttt{STORAGE}} }

% christos 2, 4
% vasilis 1, 3

\usepackage{amsmath}
\usepackage{subfig}

\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocodex}
\newcommand{\algorithmautorefname}{Algorithm}

\begin{document}

\title{Big Data Analytics with Scala and Apache Spark}
\subtitle{M.Sc. course on ``Technologies for Big Data Analysis'' - Assignment 3}

\author{Christos Balaktsis (506)}
\email{balaktsis@csd.auth.gr}
\author{Vasileios Papastergios (505)}
\email{papster@csd.auth.gr}
\affiliation{
  \institution{Aristotle University}
  \city{Thessaloniki}
  \country{Greece}
}

\renewcommand{\shortauthors}{C. Balaktsis and V. Papastergios}
\maketitle

\section{Introduction}

The current document is a technical report for the third programming assignment in the M.Sc. course on
\emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc Program}\footnote{https://dws.csd.auth.gr/} of the Aristotle University of Thessaloniki, Greece. The course is taught by Professor Apostolos Papadopoulos~\footnote{https://datalab-old.csd.auth.gr/$\sim$apostol/}. The authors attended the course during their first year of Ph.D. studies at the Institution.

The assignment contains 4 sub-problems and is part of a series, comprising 3 programming assignments on the following topics:
\begin{description}
  \item[Assignment 1] Multi-threading Programming and Inter-Process Communication
  \item[Assignment 2] The Map-Reduce Programming Paradigm
  \item[Assignment 3] Big Data Analytics with Scala and Apache Spark
\end{description}
In this document we focus on Assignment 3 and its 4 sub-problems.
We refer to them as \emph{problems} in the rest of the document for simplicity.
The source code of our solution has been made available at the following public repository in the GitHub platform: \href{https://github.com/Bilpapster/big-data-playground}{\texttt{\small https://github.com/Bilpapster/big-data-playground}}.

\textbf{Roadmap}.
The rest of our work is structured as follows.
We devote one section to each one of the 4 problems.
That means problems 1, 2, 3 and 4 are presented in~\autoref{sec:problem1},~\autoref{sec:problem2},~\autoref{sec:problem3} and~\autoref{sec:problem4} respectively.
For each problem, we first provide the problem statement, as given by the assignment.
Next, we thoroughly present the reasoning and/or methodology we have adopted to approach the problem and devise a solution.
Wherever applicable, we also provide insights about the source code implementation we have developed.
Finally, we conclude our work in~\autoref{sec:conclusion}.
The appendix includes the evaluation results for any issues that necessitated them.

\section{Problem 1: Numeronyms}
\label{sec:problem1}
We discuss here the first problem of the assignment.
The main target of the assignment is to get familiar with a simple task leveraging Apache Spark and Scala programming language. This is a WordCount problem's variation.

\subsection{Problem Statement}
Implement an \textbf{Apache Spark} (Scala) program, a variation of the \textbf{word-count} problem, to compute the average length of words that start with a specific letter (a-z). The program should sort the results based on the average length, printing first the letters with higher average length. For example:
\begin{description}
  \item k – $8.2$
  \item a – $5.6$
  \item b – $4.8$
\end{description}
Note that you may opt for preprocessing the input text, e.g., transforming all letters to lowercase or ignoring words that start with a number.

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Scala programming language version 2.13.15 and Apache Spark version 3.5.3. We have used SBT as the build tool of our solution.
We have used the Software Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using SBT as the build tool. All dependencies of the project can be found in the \texttt{build.sbt} file located at the root folder of the project.
The project is compiled and executed directly from the IntelliJ IDEA.

To run the project, open the \texttt{src/main/scala/Task1WordLength.scala} file in IntelliJ IDEA, and execute the main method. After successful execution of the program, the results can be viewed under the \texttt{output/task1} directory.

\subsubsection{Implementation}
The proposed approach leverages the Apache Spark computing framework to calculate the average word length for each one of the 26 letters in the Latin alphabet. In particular, the input text is read and tokenized into words. As part of a preprocessing step, all letters are transformed to lowercase and all words that start with a number are dropped.

In order to compute the average length for each initial letter, we leverage the \texttt{RDD} API of Apache Spark. More specifically, we use a \texttt{PairRDD} that stores composite key-value pairs in the form \texttt{(letter, (word\_length, 1))}. The key of each pair is the initial letter of the respective word (\texttt{letter}). The value of each pair is a pair itself, having as (false) key the length of the word (\texttt{word\_length}) and as (false) value a unary value (\texttt{1}). We use the term \emph{false} for the key and value of the inner pair, since we do not utilize them as an actual key-value pair. As a matter of fact, we treat the inner pair as a plain tuple of values,.

Subsequently, we group all pairs by key (i.e., initial letter) and compute the following two intermediate results \emph{per initial letter}:
\begin{enumerate}
  \item sum of word lengths that start with the respective letter, by adding up all false keys of the inner pair, and
  \item total number of words that start with the respective letter, by adding up all unary values (false values) of the inner pair.
\end{enumerate}

Having computed these intermediate results for each initial letter, the average length can be computed as the fraction
\begin{displaymath}
  average~word~length_{~letter} = \frac{sum~of~word~lengths_{~letter}}{total~number~of~words_{~letter}},~ \forall letter \in Latin~Alphabet
\end{displaymath}

As a final step, the results are sorted in descending order w.r.t. the average length and saved to a text file. By default, after successful execution of the program, the results can be found under the \texttt{output/task1} directory.

\subsubsection{Evaluation}
The experiments were executed using the dataset \texttt{SherlockHolmes.txt}. The results are listed below in a 5-column format and graphically depicted in~\autoref{img:histogram:word_lengths}.
\begin{multicols}{4}
  \noindent
  c:7.19
  \\ e: 7.11
  \\ q: 7.01
  \\ p: 7.01
  \\ r: 6.87
  \\ d: 6.37
  \\ v: 5.98
  \\ g: 5.93
  \\ s: 5.8
  \\ z: 5.74
  \\ u: 5.61
  \\ j: 5.59
  \\ k: 5.37
  \\ l: 5.34
  \\ f: 5.17
  \\ m: 5.14
  \\ n: 4.81
  \\ b: 4.54
  \\ w: 4.28
  \\ h: 3.81
  \\ a: 3.74
  \\ y: 3.72
  \\ t: 3.64
  \\ i: 3.46
  \\ x: 3.41
  \\ o: 3.01
  \label{multicol:test}
\end{multicols}

% import matplotlib.pyplot as plt
% import matplotlib
% import numpy as np

% data = [ 7.19, 7.11, 7.01, 7.01, 6.87, 6.37, 5.98, 5.93, 5.8, 5.74, 5.61, 5.59, 5.37, 5.34, 5.17, 5.14, 4.81, 4.54, 4.28, 3.81, 3.74, 3.72, 3.64, 3.46, 3.41, 3.01]

% letters = ['c', 'e', 'q', 'p', 'r', 'd', 'v', 'g', 's', 'z', 'u', 'j', 'k', 'l', 'f', 'm', 'n', 'b', 'w', 'h', 'a', 'y', 't', 'i', 'x', 'o']

% font = {'family' : 'normal',
%         'weight' : 'normal',
%         'size'   : 30}

% matplotlib.rc('font', **font)

% plt.figure(figsize=(30, 10))
% plt.bar(letters, data, color='orange', alpha=0.7)
% plt.title('Word length Histogram')
% plt.xlabel('Letter')
% plt.ylabel('Average word length')
% plt.axhline(y = np.mean(data), color = 'r', linestyle = '-.', label="Mean average word length")
% plt.legend()
% plt.show()

\begin{figure}[tb!]
  \centering
  \includegraphics[width=\linewidth]{figures/download}
  \caption{A histogram depicting the average word length (y-axis) for every letter of the Latin alphabet (x-axis). The mean average length is shown as a horizontal red dashed line.}
  \label{img:histogram:word_lengths}
\end{figure}

\section{Problem 2: Movie Analytics}
\label{sec:problem2}
The second problem focuses on producing analytic insights from an IMDB dataset.


\subsection{Problem Statement}
Implement a \textbf{MapReduce} program to perform analytics tasks on an IMDB dataset about movies. The utter goal of your analysis would be to extract useful insights from the available movie data that will assist the IMDB team provide better recommendations for movies, based on their genre and/or country. In particular, the dataset (\texttt{movies.csv}) contains the following fields:
\begin{itemize}
  \item imdbID: unique identifier of the movie in the IMDB database
  \item title: the movie title
  \item year: the year the movie was first released
  \item duration: the duration of the movie
  \item genre(s): the genre or genres in which the movie is classified
  \item premier date: the date of the first showing of the movie
  \item score: the IMDB score of the movie
  \item country/-ies: the country or countries the movie was produced in
\end{itemize}

You are asked to implement Map-Reduce source code in Java programming language for the following analytics tasks:
\begin{itemize}
  \item Calculate the total duration of all movies per country. Note that in case multiple countries are recorded for a movie, the respective duration should be counted for all of them separately.
  \item Calculate the total number of movies per year and genre, having IMDB score over 8. For movies that have more than one genre, the sum should be separate for each genre.
\end{itemize}

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
We have used the Java Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
the repository.
The project is compiled and executed directly from IntelliJ IDEA.

To run the project, open the \texttt{MovieAnalyticsMaster} class in IntelliJ IDEA, and execute the main method.
You will need to specify the following command-line arguments:

\begin{itemize}
  \item \texttt{<input\_path>} specifies the directory containing the input text files, located at \texttt{map-reduce/movieAnalytics/ input}.
  \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be created in the \texttt{out} folder.
\end{itemize}

IntelliJ IDEA will handle the compilation and execution automatically when the main method is run. Make sure to configure the input and output paths as required for your specific run. Note that the command-line arguments must follow the specified order and format. If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

\subsubsection{Implementation}
The proposed approach leverages the MapReduce programming paradigm to compute the analytical insights for the two tasks. \autoref{img:movieAnalyticsSolutionDiagram} depicts the architecture of our solution as a diagram. We opt for executing two separate jobs; one for each task presented by the problem statement.

\begin{figure}[tb!]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/movieAnalytics}
  \caption{The MapReduce solution architecture for the movie analytics problem. Two separate jobs are executed to solve the two tasks, namely duration per country (green) and movies per year \& genre (yellow). The two tasks also create separate subdirectories for writing the results.}
  \label{img:movieAnalyticsSolutionDiagram}
\end{figure}

\paragraph{Task 1: Duration per country} One map-reduce cycle is enough to handle this task, as depicted in the top part of~\autoref{img:movieAnalyticsSolutionDiagram} in green color. The map function parses the CSV file line by line and extracts the useful fields from each line. In this case, the useful fields are the country (or countries) the movie was produced and the movie duration, i.e., the fourth and ninth fields in the input line respectively. We employ more complex logic to handle cases where there are multiple countries in a single movie. In particular, we parse again the field and tokenize it into the separate countries, producing a key-value pair for each country-duration pair within a movie. The reduce function is, then, trivial; it just adds up the durations (value) per country (key) and outputs the results. The interested reader can refer to the \texttt{CSVProcessor.java} (mapper), \texttt{AnayticsEngine.java} (reducer) and \texttt{MovieAnalyticsMaster.java} (driver) files for the source code implementation of our solution.

\paragraph{Task 2: Movies per year \& genre w.r.t. score constraint} The task is similar to the first one, with the only difference lying in the fields extracted from the input CSV line. In this case, we are interested about the year, the genre (or genres) and score fields, i.e., the third, fifth and seventh fields in the input CSV line. The map function produces key-value pairs in the form \texttt{(composite\_key, 1)}, where the composite key consists of the year and the genre of the respective movie concatenated by an underscore, e.g., \texttt{2024\_action}. We handle multiple genres per movie similarly with the multiple countries in task 1. We employ the same trivial reducer that adds up the values (ones) per (composite) key, as shown in the bottom part of~\autoref{img:movieAnalyticsSolutionDiagram} in yellow color.

\subsubsection{Evaluation} Our solution is tested using the provided IMDB dataset, namely \texttt{movies.csv}. We list the execution results in~\autoref{sec:data2}.

\section{Problem 3: DNA Sequence Patterns}
\label{sec:problem3}
The third problem focuses on mining unstructured data, namely DNA sequences, encoded in text format.


\subsection{Problem Statement}
A DNA sequence consists of four distinct symbols, namely A, G, C, and T. For instance, the following lines represent a part of the DNA sequence of the E. Coli bacterium~\footnote{\href{https://en.wikipedia.org/wiki/Pathogenic_Escherichia_coli}{https://en.wikipedia.org/wiki/Pathogenic\_Escherichia\_coli}}:

\begin{verbatim}
AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC
TTCTGAACTGGTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAA
TATAGGCATAGCGCACAGACAGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACC
ATTACCACCACCATCACCATTACCACAGGTAACGGTGCGGGCTGACGCGTACAGGAAACACAGAAAAAAG
\end{verbatim}

Implement a \textbf{MapReduce} program that computes the frequency (i.e., number of occurrences) of all subsequences of lengths 2, 3, and 4 that are present in the input DNA sequence. Note that the processing of each line should be independent of all other lines. Use the file \texttt{ecoli.txt} to test your solution.

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
We have used the Java Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
the repository.
The project is compiled and executed directly from IntelliJ IDEA.
To run the project, open the \texttt{DNASequenceAnalyticsMaster} class in IntelliJ IDEA, and execute the main method.
You will need to specify the following command-line arguments:

\begin{itemize}
  \item \texttt{<input\_path>} specifies the directory containing the input text files, located at \texttt{map-reduce/ \\ dnaSequencePatterns/input}.
  \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be created in the \texttt{out} folder.
\end{itemize}

IntelliJ IDEA will handle the compilation and execution automatically when the main method is run. Make sure to configure the input and output paths as required for your specific run. Note that the command-line arguments must follow the specified order and format. If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

\subsubsection{Implementation} \autoref{algo:subsequences} summarizes the most substantial part of our implementation, namely the logic that is implemented inside the mappers of our solution. At every execution of the map function, a DNA sequence line is given as input to the mapper. The latter is responsible for populating all possible subsequences of length 2, 3 and 4. We employ a sliding window that is nested inside a for-loop defining the subsequence length. The source code implementation of~\autoref{algo:subsequences} can be found in the \texttt{DNASequenceProcessor.java} file.

\begin{algorithm}[!tb]
  \caption{Populating DNA subsequences}
  \label{algo:subsequences}
  \begin{algorithmic}[1]
    \LComment{Input: a DNA sequence $line$ (variable line)}
    \For{$length \in \{2, 3, 4\}$}
    \State Create a sliding window $window$ of length $length$.
    \State Place left end of $window$ at the start of $line$.
    \While{right end of $window$ has not exceeded $line$}
    \State $subsequence \gets$ contents of $window$
    \State Yield $(subsequence, 1)$ key-value pair.
    \State Slide window to the right by $1$.
    \EndWhile
    \EndFor
  \end{algorithmic}
\end{algorithm}

\subsubsection{Evaluation} Our solution is tested using the provided DNA sequence, namely \texttt{ecoli.txt}. We list the execution results in~\autoref{sec:data3}.

\section{Problem 4: Probabilistic graph}
\label{sec:problem4}
We discuss here the fourth problem of the assignment.
The main target of the assignment is to get acquainted with performing various MapReduce phases on a pipeline frame.

\subsection{Problem Statement}
We are given an input file in text format where each line contains a connection between two vertices of a network and a
probability value.
For each edge \( e \), there is a probability value \( p(e) \) which indicates the probability that the two vertices are
connected by the edge.
Obviously, the values of \( p(e) \) range between 0 and 1.
The values in each line are separated by a space.
Consider the network shown in the previous figure.
The edge connecting vertices 4 and 5 has a probability of
0.8, the edge connecting vertices 2 and 3 has a probability of 0.2, etc.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.25\linewidth]{figures/graph}
\end{figure}

The file corresponding to this graph would be:

\[
  1 \ 2 \ 0.6, \\
  1 \ 3 \ 0.9, \\
  2 \ 3 \ 0.2, \\
  3 \ 4 \ 0.75, \\
  2 \ 5 \ 0.2, \\
  4 \ 5 \ 0.8,
\]

The edges are generally stored in random order in the file, so we cannot assume they have a specific arrangement.

The following tasks are requested:

\begin{enumerate}
  \item Write a Java program that computes the average degree for all the vertices.
  \item The average degree is defined as the sum of the probabilities of the edges that fall on a vertex.
  \item For example, in the previous diagram, the average degree of vertex 3 is \( 0.9 + 0.2 + 0.75 = 1.85 \).
  \item Before performing this calculation, you should ignore all edges with a probability less than a threshold \( T \)
        , which should be passed as a parameter to the main function.
  \item Modify the code from task 1 so that, at the end, only the vertices with an average degree greater than the
        average of the degrees of all the vertices are displayed in the output.

\end{enumerate}

\subsection{Proposed approach}
\subsubsection{Setting}
Our implementation is run and tested in a Linux environment with 12 cores, using the Java programming language.
We have used the Java Development Kit (JDK) version 11.0.11.
The source code is developed in IntelliJ IDEA Community Edition 2021.1.1 and managed using Maven as the build tool.

The project’s dependencies, including Hadoop libraries, are defined in the \texttt{pom.xml} file located in the root of
the repository.
The project is compiled and executed directly from IntelliJ IDEA.

To run the project, open the \texttt{GraphMaster} class in IntelliJ IDEA, and execute the main method.
You will need to specify the following command-line arguments:

\begin{itemize}
  \item \texttt{<input\_path>} specifies the directory containing the input text files, located at
        \texttt{map-reduce/ probabilisticGraph /input}.
  \item \texttt{<output\_path>} specifies the directory where the MapReduce output will be written, which will be
        created in the \texttt{out} folder.
  \item \texttt{<T>} is the minimum edge-degree threshold for vertices to be included in the results.
\end{itemize}

IntelliJ IDEA will handle the compilation and execution automatically when the main method is run.
Make sure to configure the input and output paths as required for your specific run.

Note that the command-line arguments must follow the specified order and format.
If any of the arguments are missing or invalid, the program will terminate with an appropriate error message.

\subsubsection{Implementation}
The problem at hand involves processing a probabilistic graph, where each edge connects two vertices with a probability
value.
The task is to compute the average degree for each vertex, considering only edges whose probability exceeds a given
threshold \( T \).
The methodology proposed here utilizes a distributed MapReduce framework, implemented using Hadoop, to efficiently
process and analyze large-scale graph data.
The approach is divided into three main phases, each handled by separate MapReduce jobs, with intermediate results
passed between the phases.

The \texttt{GraphMaster} class serves as the orchestrator of the entire MapReduce process.
It manages the execution of the three phases, invoking the appropriate MapReduce jobs in sequence.
The main steps executed by \texttt{GraphMaster} are:

\begin{enumerate}
  \item It retrieves the command-line arguments, including the input and output directories as well as the threshold
        value \( T \), which determines which edges to consider in the graph.
  \item It initiates the first MapReduce job to compute the degree of each vertex in the graph.
        The input consists of edge data, and the output is a summation of edge probabilities for each vertex.
  \item After the first job completes, the \texttt{GraphMaster} starts the second MapReduce job to calculate the mean
        degree of the graph, using the results from the first job.
  \item Once the mean degree is computed, \texttt{GraphMaster} starts the third MapReduce job, which filters out the
        vertices with degrees lower than the mean degree.
  \item It cleans up intermediate directories after each phase and moves the final output to the desired location.
\end{enumerate}

\texttt{GraphMaster} coordinates these steps by configuring and executing the MapReduce jobs, ensuring that each phase
depends on the results of the previous one. \\

The entire process consists of three MapReduce jobs, executed sequentially:

\begin{enumerate}
  \item \textbf{Phase 1:} The first job computes the degree of each vertex by summing the probabilities of the edges
        connected to it.
        It produces intermediate results for each vertex.
  \item \textbf{Phase 2:} The second job calculates the mean degree by summing the degrees from the first phase and
        dividing by the total number of vertices.
  \item \textbf{Phase 3:} The third job filters out the vertices with a degree lower than the mean degree and produces
        the final filtered result.
\end{enumerate}

After each job completes, intermediate data is written to disk and passed as input to the subsequent job.
The final output consists of the vertices whose degree exceeds or equals the mean degree. \\

\textbf{Phase 1: Calculating Vertex Degree}
The first MapReduce job is responsible for calculating the degree of each vertex in the graph, where the degree is
defined as the sum of the probabilities of the edges connected to that vertex.
The degree of each vertex is computed by the \texttt{GraphMapper} and \texttt{GraphReducer} classes.

The \texttt{GraphMapper} class reads each edge from the input data, which consists of lines containing two vertices
and the associated probability value.
It splits each line into two components (the two vertices), and for each vertex, it emits the corresponding probability
value as the output.
The mapper also checks if the probability value exceeds the threshold \( T \), which is provided as a configuration
parameter.
If the probability is greater than or equal to \( T \), it emits the vertex along with the corresponding probability.

The \texttt{GraphReducer} class receives the vertex and associated probability values emitted by the mapper.
For each vertex, it sums up all the probabilities of the edges connected to that vertex, and emits the pair as output.

\textbf{Phase 2: Calculating the Mean Degree}
The second phase of the approach calculates the mean degree of all vertices.
This is done by summing the degree values from the previous phase and dividing by the total number of vertices.

The \texttt{MeanMapper} class receives the degree values from the output of the first MapReduce job.
It emits two key-value pairs for each input line: a count of the number of vertices and the sum of the degree values.
These are emitted with fixed keys (\texttt{"count"} and \texttt{"sum"}) so that they can be aggregated in the reducer.

The \texttt{MeanReducer} class processes the count and sum values emitted by the mapper.
It calculates the total sum and count of vertices and then writes these values as output.
The mean degree is calculated by dividing the sum by the count, and the result is stored for use in the next phase.

\textbf{Phase 3: Filtering Vertices by Mean Degree}
In the third phase, the vertices whose degree is greater than or equal to the mean degree calculated in Phase 2 are
selected.
This phase uses the results from Phase 1 and Phase 2, applying the filtering criteria to retain only those vertices with
a degree greater than or equal to the mean.

The \texttt{FilterMapper} class takes the degree values emitted by the first reducer and writes them as key-value pairs.
Each key is a vertex, and the value is the degree of that vertex.
The mapper does not perform any filtering; instead, it simply passes the values along to the reducer.

The \texttt{FilterReducer} class filters out the vertices whose degree is less than the mean degree.
It retrieves the mean degree from the configuration, and for each vertex, it compares the degree value to the mean.
If the degree is greater than or equal to the mean, it emits the vertex along with its degree.
This final output contains only the vertices that satisfy the filtering criteria.

\subsubsection{Evaluation}
The experiments were executed using the dataset `collins.txt` with a parameter setting of \( T = 0.8 \).

To provide a comprehensive overview, the results of the experiment, including the detailed values derived from the
dataset, are presented in~\autoref{sec:data4} in a 2-column format.



\section{Conclusion}
\label{sec:conclusion}
In this document we have presented our solutions and rationale for solving the second assignment of the M.Sc.
course on \emph{Technologies for Big Data Analysis}, offered by the \emph{DWS M.Sc. Program}. For each one of the four problems of the assignment, we have presented their statement, as well as the solution approach we have adopted. All execution results with the provided datasets can be found in the appendices of our work.

\newpage
\appendix
\section{Problem 1 execution results}
\label{sec:data1}
\input{appendix1}

\section{Problem 2 execution results}
\label{sec:data2}
\input{appendix2}

\section{Problem 3 execution results}
\label{sec:data3}
\input{appendix3}

\subsection{Problem 4 execution results}
\label{sec:data4}
\input{appendix4}


\end{document}
\endinput

